{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c15f53",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb84082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'transformer-slt/')\n",
    "from onmt.translate.translator import build_translator\n",
    "import onmt\n",
    "import onmt.inputters as inputters\n",
    "from onmt.translate.beam_search import BeamSearch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_class(name):\n",
    "    components = name.rsplit('.', 1)\n",
    "    print(components)\n",
    "    mod = importlib.import_module(components[0])\n",
    "    mod = getattr(mod, components[1])\n",
    "    print(mod)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd55f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n",
      "['slr_network', 'SLRModel']\n",
      "<class 'slr_network.SLRModel'>\n"
     ]
    }
   ],
   "source": [
    "model_args = {\"num_classes\": 1296, \"c2d_type\": \"resnet18\", \"conv_type\": 2, \"use_bn\": 1}\n",
    "loss_weights = {\"ConvCTC\": 1.0, \"SeqCTC\": 1.0, \"Dist\": 10.0}\n",
    "gloss_dict = np.load('/home/aayush/Thesis/VAC_CSLR/preprocess/phoenix2014/gloss_dict.npy', allow_pickle=True)\\\n",
    "                .item()\n",
    "print(len(gloss_dict) + 1)\n",
    "\n",
    "model_class = import_class(\"slr_network.SLRModel\")\n",
    "model = model_class(**model_args,\n",
    "                    gloss_dict=gloss_dict,\n",
    "                    loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696966db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLRModel(\n",
      "  (conv2d): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (conv1d): TemporalConv(\n",
      "    (temporal_conv): Sequential(\n",
      "      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
      "      (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (fc): Linear(in_features=1024, out_features=1296, bias=True)\n",
      "  )\n",
      "  (temporal_model): BiLSTMLayer(\n",
      "    (rnn): LSTM(1024, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=1296, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a34edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_weights(state_dict, modified=False):\n",
    "    state_dict = OrderedDict([(k.replace('.module', ''), v) for k, v in state_dict.items()])\n",
    "    if not modified:\n",
    "        return state_dict\n",
    "    modified_dict = dict()\n",
    "    return modified_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c54e3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('model_new_V2.0_34.20.pt', map_location=torch.device('cpu'))\n",
    "weights = modified_weights(state_dict['model_state_dict'], False)\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "689af802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+------------+\n",
      "|                 Modules                 | Parameters |\n",
      "+-----------------------------------------+------------+\n",
      "|           conv2d.conv1.weight           |    9408    |\n",
      "|            conv2d.bn1.weight            |     64     |\n",
      "|             conv2d.bn1.bias             |     64     |\n",
      "|       conv2d.layer1.0.conv1.weight      |   36864    |\n",
      "|        conv2d.layer1.0.bn1.weight       |     64     |\n",
      "|         conv2d.layer1.0.bn1.bias        |     64     |\n",
      "|       conv2d.layer1.0.conv2.weight      |   36864    |\n",
      "|        conv2d.layer1.0.bn2.weight       |     64     |\n",
      "|         conv2d.layer1.0.bn2.bias        |     64     |\n",
      "|       conv2d.layer1.1.conv1.weight      |   36864    |\n",
      "|        conv2d.layer1.1.bn1.weight       |     64     |\n",
      "|         conv2d.layer1.1.bn1.bias        |     64     |\n",
      "|       conv2d.layer1.1.conv2.weight      |   36864    |\n",
      "|        conv2d.layer1.1.bn2.weight       |     64     |\n",
      "|         conv2d.layer1.1.bn2.bias        |     64     |\n",
      "|       conv2d.layer2.0.conv1.weight      |   73728    |\n",
      "|        conv2d.layer2.0.bn1.weight       |    128     |\n",
      "|         conv2d.layer2.0.bn1.bias        |    128     |\n",
      "|       conv2d.layer2.0.conv2.weight      |   147456   |\n",
      "|        conv2d.layer2.0.bn2.weight       |    128     |\n",
      "|         conv2d.layer2.0.bn2.bias        |    128     |\n",
      "|   conv2d.layer2.0.downsample.0.weight   |    8192    |\n",
      "|   conv2d.layer2.0.downsample.1.weight   |    128     |\n",
      "|    conv2d.layer2.0.downsample.1.bias    |    128     |\n",
      "|       conv2d.layer2.1.conv1.weight      |   147456   |\n",
      "|        conv2d.layer2.1.bn1.weight       |    128     |\n",
      "|         conv2d.layer2.1.bn1.bias        |    128     |\n",
      "|       conv2d.layer2.1.conv2.weight      |   147456   |\n",
      "|        conv2d.layer2.1.bn2.weight       |    128     |\n",
      "|         conv2d.layer2.1.bn2.bias        |    128     |\n",
      "|       conv2d.layer3.0.conv1.weight      |   294912   |\n",
      "|        conv2d.layer3.0.bn1.weight       |    256     |\n",
      "|         conv2d.layer3.0.bn1.bias        |    256     |\n",
      "|       conv2d.layer3.0.conv2.weight      |   589824   |\n",
      "|        conv2d.layer3.0.bn2.weight       |    256     |\n",
      "|         conv2d.layer3.0.bn2.bias        |    256     |\n",
      "|   conv2d.layer3.0.downsample.0.weight   |   32768    |\n",
      "|   conv2d.layer3.0.downsample.1.weight   |    256     |\n",
      "|    conv2d.layer3.0.downsample.1.bias    |    256     |\n",
      "|       conv2d.layer3.1.conv1.weight      |   589824   |\n",
      "|        conv2d.layer3.1.bn1.weight       |    256     |\n",
      "|         conv2d.layer3.1.bn1.bias        |    256     |\n",
      "|       conv2d.layer3.1.conv2.weight      |   589824   |\n",
      "|        conv2d.layer3.1.bn2.weight       |    256     |\n",
      "|         conv2d.layer3.1.bn2.bias        |    256     |\n",
      "|       conv2d.layer4.0.conv1.weight      |  1179648   |\n",
      "|        conv2d.layer4.0.bn1.weight       |    512     |\n",
      "|         conv2d.layer4.0.bn1.bias        |    512     |\n",
      "|       conv2d.layer4.0.conv2.weight      |  2359296   |\n",
      "|        conv2d.layer4.0.bn2.weight       |    512     |\n",
      "|         conv2d.layer4.0.bn2.bias        |    512     |\n",
      "|   conv2d.layer4.0.downsample.0.weight   |   131072   |\n",
      "|   conv2d.layer4.0.downsample.1.weight   |    512     |\n",
      "|    conv2d.layer4.0.downsample.1.bias    |    512     |\n",
      "|       conv2d.layer4.1.conv1.weight      |  2359296   |\n",
      "|        conv2d.layer4.1.bn1.weight       |    512     |\n",
      "|         conv2d.layer4.1.bn1.bias        |    512     |\n",
      "|       conv2d.layer4.1.conv2.weight      |  2359296   |\n",
      "|        conv2d.layer4.1.bn2.weight       |    512     |\n",
      "|         conv2d.layer4.1.bn2.bias        |    512     |\n",
      "|      conv1d.temporal_conv.0.weight      |  2621440   |\n",
      "|       conv1d.temporal_conv.0.bias       |    1024    |\n",
      "|      conv1d.temporal_conv.1.weight      |    1024    |\n",
      "|       conv1d.temporal_conv.1.bias       |    1024    |\n",
      "|      conv1d.temporal_conv.4.weight      |  5242880   |\n",
      "|       conv1d.temporal_conv.4.bias       |    1024    |\n",
      "|      conv1d.temporal_conv.5.weight      |    1024    |\n",
      "|       conv1d.temporal_conv.5.bias       |    1024    |\n",
      "|             conv1d.fc.weight            |  1327104   |\n",
      "|              conv1d.fc.bias             |    1296    |\n",
      "|     temporal_model.rnn.weight_ih_l0     |  2097152   |\n",
      "|     temporal_model.rnn.weight_hh_l0     |  1048576   |\n",
      "|      temporal_model.rnn.bias_ih_l0      |    2048    |\n",
      "|      temporal_model.rnn.bias_hh_l0      |    2048    |\n",
      "| temporal_model.rnn.weight_ih_l0_reverse |  2097152   |\n",
      "| temporal_model.rnn.weight_hh_l0_reverse |  1048576   |\n",
      "|  temporal_model.rnn.bias_ih_l0_reverse  |    2048    |\n",
      "|  temporal_model.rnn.bias_hh_l0_reverse  |    2048    |\n",
      "|     temporal_model.rnn.weight_ih_l1     |  2097152   |\n",
      "|     temporal_model.rnn.weight_hh_l1     |  1048576   |\n",
      "|      temporal_model.rnn.bias_ih_l1      |    2048    |\n",
      "|      temporal_model.rnn.bias_hh_l1      |    2048    |\n",
      "| temporal_model.rnn.weight_ih_l1_reverse |  2097152   |\n",
      "| temporal_model.rnn.weight_hh_l1_reverse |  1048576   |\n",
      "|  temporal_model.rnn.bias_ih_l1_reverse  |    2048    |\n",
      "|  temporal_model.rnn.bias_hh_l1_reverse  |    2048    |\n",
      "|            classifier.weight            |  1327104   |\n",
      "|             classifier.bias             |    1296    |\n",
      "+-----------------------------------------+------------+\n",
      "Total Trainable Params: 34303072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34303072"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2a5c41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SLRModel(\n",
       "  (conv2d): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (conv1d): TemporalConv(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1296, bias=True)\n",
       "  )\n",
       "  (temporal_model): BiLSTMLayer(\n",
       "    (rnn): LSTM(1024, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1296, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094640a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a28ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im westen und nordosten bleibt es trocken .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'im westen und nordosten bleibt es trocken .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample try\n",
    "temp = prediction_after_transformer('WEST UND loc-NORDOST TROCKEN __OFF__ __ON__')\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b760f13",
   "metadata": {},
   "source": [
    "# Input format modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76fbe703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from utils import video_augmentation\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f0fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img_path, dsize='256x256px'): #852x480px\n",
    "    dsize = tuple(int(res) for res in re.findall(\"\\d+\", dsize))\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, dsize, interpolation=cv2.INTER_LANCZOS4)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize_img_folder(frames_folder_path):\n",
    "    img_list = glob.glob(frames_folder_path)\n",
    "    for img_path in img_list:\n",
    "        rs_img = resize_img(img_path)\n",
    "        cv2.imwrite(img_path, rs_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf1e75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleBaseFeeder(data.Dataset):\n",
    "    \n",
    "    def __init__(self, prefix, gloss_dict):\n",
    "        self.prefix = prefix # image frames path : ./01April_2010_Thursday_heute_default-5/1/*.png\n",
    "        self.dict = gloss_dict\n",
    "        self.data_aug = self.transform()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_data, label, fi = self.read_video(idx)\n",
    "        input_data, label = self.normalize(input_data, label)\n",
    "        \n",
    "        return input_data, torch.LongTensor(label), ''#self.inputs_list[idx]['original_info']\n",
    "\n",
    "    \n",
    "    def read_video(self, index, num_glosses=-1):\n",
    "        # load file info        \n",
    "        img_folder = os.path.join(self.prefix)\n",
    "        img_list = sorted(glob.glob(img_folder))\n",
    "        label_list = []\n",
    "                \n",
    "        return [cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in img_list],\\\n",
    "            label_list, {}\n",
    "\n",
    "    \n",
    "    def normalize(self, video, label, file_id=None):\n",
    "        video, label = self.data_aug(video, label, file_id)\n",
    "        video = video.float() / 127.5 - 1\n",
    "        return video, label\n",
    "\n",
    "    \n",
    "    def transform(self):\n",
    "        print(\"Apply testing transform.\")\n",
    "        return video_augmentation.Compose([\n",
    "            #video_augmentation.Resize(0.5, 'bicubic'),\n",
    "            video_augmentation.CenterCrop(224),\n",
    "            video_augmentation.ToTensor(),\n",
    "        ]) \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = [item for item in sorted(batch, key=lambda x: len(x[0]), reverse=True)]\n",
    "        video, label, info = list(zip(*batch))\n",
    "        if len(video[0].shape) > 3:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([np.int(np.ceil(len(vid) / 4.0)) * 4 + 12 for vid in video])\n",
    "            left_pad = 6\n",
    "            right_pad = int(np.ceil(max_len / 4.0)) * 4 - max_len + 6\n",
    "            max_len = max_len + left_pad + right_pad\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid[0][None].expand(left_pad, -1, -1, -1),\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video)\n",
    "        else:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([len(vid) for vid in video])\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid), -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video).permute(0, 2, 1)\n",
    "        label_length = torch.LongTensor([len(lab) for lab in label])\n",
    "        if max(label_length) == 0:\n",
    "            return padded_video, video_length, [], [], info\n",
    "        else:\n",
    "            padded_label = []\n",
    "            for lab in label:\n",
    "                padded_label.extend(lab)\n",
    "            padded_label = torch.LongTensor(padded_label)\n",
    "            return padded_video, video_length, padded_label, label_length, info\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1 # as for prediction we just have one folder/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34e5785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1295"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gloss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ea7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(frames_folder_path, gloss_dict):\n",
    "    \n",
    "    pred_dataset = SampleBaseFeeder(frames_folder_path, gloss_dict)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                pred_dataset,\n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                num_workers=4, \n",
    "                collate_fn=pred_dataset.collate_fn,\n",
    "            )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ffa2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_after_transformer(glosses):\n",
    "    opt = Namespace(align_debug=False, alpha=0.0, attn_debug=False, avg_raw_probs=False,\n",
    "                    batch_size=30, batch_type='sents', beam_size=4, beta=-0.0,\n",
    "                    block_ngram_repeat=0, config=None, coverage_penalty='none', data_type='text',\n",
    "                    dump_beam='', dynamic_dict=False, fp32=False, gpu=-1, ignore_when_blocking=[],\n",
    "                    image_channel_size=3, length_penalty='none', log_file='', log_file_level='0',\n",
    "                    max_length=100, max_sent_length=None, min_length=0, models=['transformer-slt/current_model.pt'],\n",
    "                    n_best=1, output=None, phrase_table='', random_sampling_temp=1.0,\n",
    "                    random_sampling_topk=1, ratio=-0.0, replace_unk=True, report_align=False,\n",
    "                    report_time=False, sample_rate=16000, save_config=None, seed=829,\n",
    "                    shard_size=10000, share_vocab=False, src=glosses,\n",
    "                    src_dir='', stepwise_penalty=False, tgt=None, verbose=False, window='hamming',\n",
    "                    window_size=0.02, window_stride=0.01)\n",
    "\n",
    "    translator = build_translator(opt, report_score=True, out_file='')\n",
    "\n",
    "    load_test_model = onmt.model_builder.load_test_model\n",
    "    fields, model, model_opt = load_test_model(opt)\n",
    "    scorer = onmt.translate.GNMTGlobalScorer.from_opt(opt)\n",
    "\n",
    "    src_reader = inputters.str2reader[opt.data_type].from_opt(opt)\n",
    "    tgt_reader = inputters.str2reader['text'].from_opt(opt)\n",
    "\n",
    "    src_data = {\"reader\": src_reader, \"data\": [opt.src], \"dir\": ''}\n",
    "    tgt_data = {\"reader\": tgt_reader, \"data\": None, \"dir\": None}\n",
    "\n",
    "    _readers, _data, _dir = inputters.Dataset.config([('src', src_data), ('tgt', tgt_data)])\n",
    "    data = inputters.Dataset(\n",
    "            fields, readers=_readers, data=_data, dirs=_dir,\n",
    "            sort_key=inputters.str2sortkey[opt.data_type]\n",
    "            )\n",
    "\n",
    "    data_iter = inputters.OrderedIterator(\n",
    "                    dataset=data,\n",
    "                    device=torch.device(\"cpu\"),\n",
    "                    batch_size=opt.batch_size,\n",
    "                    batch_size_fn=None,\n",
    "                    train=False,\n",
    "                    sort=False,\n",
    "                    sort_within_batch=True,\n",
    "                    shuffle=False\n",
    "                )\n",
    "\n",
    "    xlation_builder = onmt.translate.TranslationBuilder(\n",
    "                            data, fields, opt.n_best, opt.replace_unk, opt.tgt,\n",
    "                            opt.phrase_table\n",
    "                        )\n",
    "\n",
    "\n",
    "    all_predictions = []\n",
    "    tgt_field = dict(fields)[\"tgt\"].base_field\n",
    "    _tgt_vocab = tgt_field.vocab\n",
    "    _tgt_eos_idx = _tgt_vocab.stoi[tgt_field.eos_token]\n",
    "    _tgt_pad_idx = _tgt_vocab.stoi[tgt_field.pad_token]\n",
    "    _tgt_bos_idx = _tgt_vocab.stoi[tgt_field.init_token]\n",
    "    _tgt_unk_idx = _tgt_vocab.stoi[tgt_field.unk_token]\n",
    "    _tgt_vocab_len = len(_tgt_vocab)\n",
    "    _exclusion_idxs = {_tgt_vocab.stoi[t] for t in opt.ignore_when_blocking}\n",
    "\n",
    "    copy_attn = model_opt.copy_attn\n",
    "\n",
    "    for batch in data_iter:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decode_strategy = BeamSearch(\n",
    "            opt.beam_size,\n",
    "            batch_size=batch.batch_size,\n",
    "            pad=_tgt_pad_idx,\n",
    "            bos=_tgt_bos_idx,\n",
    "            eos=_tgt_eos_idx,\n",
    "            n_best=opt.n_best,\n",
    "            global_scorer=scorer,\n",
    "            min_length=opt.min_length, max_length=opt.max_length,\n",
    "            return_attention=opt.attn_debug or opt.replace_unk,\n",
    "            block_ngram_repeat=opt.block_ngram_repeat,\n",
    "            exclusion_tokens=_exclusion_idxs,\n",
    "            stepwise_penalty=opt.stepwise_penalty,\n",
    "            ratio=opt.ratio)\n",
    "\n",
    "            batch_data = translator._translate_batch_with_strategy(batch, data.src_vocabs, decode_strategy)\n",
    "\n",
    "        translations = xlation_builder.from_batch(batch_data)\n",
    "        for trans in translations:\n",
    "            n_best_preds = [\" \".join(pred)\n",
    "                                    for pred in trans.pred_sents[:opt.n_best]]\n",
    "            print(n_best_preds)\n",
    "    return n_best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ceddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_from_frames(frames_folder_path, gloss_dict):\n",
    "    start = time.time()\n",
    "    loader = get_data_loader(frames_folder_path, gloss_dict)\n",
    "    for batch_idx, l_data in enumerate(loader):\n",
    "        print(l_data[0].shape)\n",
    "        device = utils.GpuDataParallel()\n",
    "        vid = device.data_to_device(l_data[0])\n",
    "        vid_lgt = device.data_to_device(l_data[1])\n",
    "        label = device.data_to_device(l_data[2])\n",
    "        label_lgt = device.data_to_device(l_data[3])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ret_dict = model(vid, vid_lgt, label=label, label_lgt=label_lgt)\n",
    "    \n",
    "    sentence = ''\n",
    "    for word, id in ret_dict['recognized_sents'][0]:\n",
    "        sentence += word + \" \"\n",
    "    \n",
    "    print(sentence)\n",
    "    if len(sentence) > 0:\n",
    "        sentence = prediction_after_transformer(sentence)[0]\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Time taken to predict (total): \", round(end - start))\n",
    "    return sentence, ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0471eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames_folder_name = '07February_2011_Monday_heute-4659'\n",
    "#frames_folder_path = './dataset/dataset/' + frames_folder_name + '/*.png'\n",
    "frames_folder_path = './recording_3/*.png'\n",
    "resize_img_folder(frames_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd96faba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply testing transform.\n",
      "torch.Size([1, 44, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 44, 3, 224, 224])\n",
      "tensor([44])\n",
      "<class 'torch.Tensor'>\n",
      "__ON__ NICHT \n",
      "['und es wird nicht ganz so ganz .']\n",
      "Time taken to predict (total):  3\n"
     ]
    }
   ],
   "source": [
    "ns, ret_dict = get_prediction_from_frames(frames_folder_path, gloss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c593f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SONNE AB BIS DANACH KOMMEN REGEN MORGEN MONTAG AUCH MEHR WOLKE WIE HEUTE FREUNDLICH VIEL \n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "for word, id in ret_dict['recognized_sents'][0]:\n",
    "    sentence += word + \" \"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d007dc0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[0.0363, 0.0363, 0.0363,  ..., 0.4332, 0.4332, 0.4332],\n",
       "          [0.1081, 0.1081, 0.1081,  ..., 0.5985, 0.5985, 0.5985],\n",
       "          [0.6660, 0.6660, 0.6660,  ..., 2.2443, 2.2443, 2.2443],\n",
       "          ...,\n",
       "          [0.0119, 0.0119, 0.0119,  ..., 0.0968, 0.0968, 0.0968],\n",
       "          [0.4354, 0.4354, 0.4354,  ..., 0.2747, 0.2747, 0.2747],\n",
       "          [0.1864, 0.1864, 0.1864,  ..., 1.2782, 1.2782, 1.2782]]]),\n",
       " 'visual_features': tensor([[[0.0000, 0.0000, 1.3646,  ..., 0.0000, 2.9663, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.6210,  ..., 0.0000, 3.6965, 0.0000]],\n",
       " \n",
       "         [[0.9466, 0.0000, 0.0000,  ..., 0.0000, 2.4295, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000, 0.4317, 0.5081,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.6179, 2.0592, 0.0890]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0137,  ..., 0.6838, 2.6322, 0.3518]]]),\n",
       " 'feat_len': tensor([34]),\n",
       " 'conv_logits': tensor([[[ -9.3633, -30.4605, -26.5280,  ..., -25.5009, -25.5422, -26.3925]],\n",
       " \n",
       "         [[-21.1860, -36.5194, -32.3841,  ..., -32.1808, -32.1922, -32.3859]],\n",
       " \n",
       "         [[-22.5708, -34.9553, -33.2950,  ..., -31.7973, -32.8888, -33.0621]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-21.3635, -42.6935, -39.3488,  ..., -37.0877, -36.1249, -37.1460]],\n",
       " \n",
       "         [[-26.7391, -42.9428, -41.1979,  ..., -39.4326, -38.1566, -39.3378]],\n",
       " \n",
       "         [[-24.5259, -40.8323, -38.6173,  ..., -37.5786, -36.7665, -38.2684]]]),\n",
       " 'sequence_logits': tensor([[[ 10.1121,  -6.0989,  -4.5725,  ...,  -4.0357,  -3.4266,  -5.2722]],\n",
       " \n",
       "         [[  5.6767,  -5.4664,  -4.2958,  ...,  -3.5843,  -3.5774,  -5.8551]],\n",
       " \n",
       "         [[  8.1606,  -5.5426,  -7.4021,  ...,  -4.5488,  -6.3940,  -7.3407]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  6.2267, -10.3777,  -8.2362,  ...,  -4.6512,  -5.0248,  -5.9107]],\n",
       " \n",
       "         [[  8.2267,  -7.7337,  -5.7163,  ...,  -4.6216,  -4.6933,  -5.4498]],\n",
       " \n",
       "         [[  7.7111,  -5.4912,  -4.1281,  ...,  -4.0437,  -4.2538,  -4.8229]]]),\n",
       " 'conv_sents': [[('SONNE', 0),\n",
       "   ('REGION', 1),\n",
       "   ('NAECHSTE', 2),\n",
       "   ('KOMMEN', 3),\n",
       "   ('REGEN', 4),\n",
       "   ('MORGEN', 5),\n",
       "   ('AUCH', 6),\n",
       "   ('MEHR', 7),\n",
       "   ('WOLKE', 8),\n",
       "   ('WIE', 9),\n",
       "   ('HEUTE', 10),\n",
       "   ('SONNE', 11)]],\n",
       " 'recognized_sents': [[('SONNE', 0),\n",
       "   ('AB', 1),\n",
       "   ('BIS', 2),\n",
       "   ('DANACH', 3),\n",
       "   ('KOMMEN', 4),\n",
       "   ('REGEN', 5),\n",
       "   ('MORGEN', 6),\n",
       "   ('MONTAG', 7),\n",
       "   ('AUCH', 8),\n",
       "   ('MEHR', 9),\n",
       "   ('WOLKE', 10),\n",
       "   ('WIE', 11),\n",
       "   ('HEUTE', 12),\n",
       "   ('FREUNDLICH', 13),\n",
       "   ('VIEL', 14)]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23cd79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26e93d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know a folder info looks like.\n",
    "\n",
    "inputs_list = np.load(f\"./preprocess/phoenix14t/test_info.npy\", allow_pickle=True).item()\n",
    "for x in inputs_list:\n",
    "    if x == 'prefix':\n",
    "        continue\n",
    "    if inputs_list[x]['fileid'] == frames_folder_name:\n",
    "        print(inputs_list[x])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6e7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f34154fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'fileid': '22November_2010_Monday_heute_default-14', 'folder': 'train/22November_2010_Monday_heute_default-14/1/*.png', 'signer': 'Signer01', 'label': '__ON__ SCHOEN ABEND MACHEN GUT TSCHUESS __OFF__', 'num_frames': 38, 'original_info': '22November_2010_Monday_heute_default-14|22November_2010_Monday_heute_default-14/1/*.png|Signer01|__ON__ SCHOEN ABEND MACHEN GUT TSCHUESS __OFF__'}\n",
      "5\n",
      "{'fileid': '30May_2011_Monday_heute_default-18', 'folder': 'train/30May_2011_Monday_heute_default-18/1/*.png', 'signer': 'Signer01', 'label': '__ON__ SCHOEN ABEND TSCHUESS __OFF__', 'num_frames': 39, 'original_info': '30May_2011_Monday_heute_default-18|30May_2011_Monday_heute_default-18/1/*.png|Signer01|__ON__ SCHOEN ABEND TSCHUESS __OFF__'}\n"
     ]
    }
   ],
   "source": [
    "inputs_list = np.load(f\"./preprocess/phoenix2014/train_info.npy\", allow_pickle=True).item()\n",
    "for x in inputs_list:\n",
    "    if x == 'prefix':\n",
    "        continue\n",
    "    \n",
    "    if inputs_list[x]['num_frames'] < 40 and 'TSCHUESS' in inputs_list[x]['label']:\n",
    "        print(len(inputs_list[x]['label'].split(' ')))\n",
    "        print(inputs_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80af755",
   "metadata": {},
   "source": [
    "# For live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00f9fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b2f014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba01b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50dd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60043963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              #mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             #mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             #)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7220800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]), lh, rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1f45d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on  frames_collection/recording_0\n",
      "Apply testing transform.\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "tensor([48])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  3\n",
      "Predicting on  frames_collection/recording_1\n",
      "Apply testing transform.\n",
      "torch.Size([1, 32, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 32, 3, 224, 224])\n",
      "tensor([32])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  2\n",
      "Predicting on  frames_collection/recording_2\n",
      "Apply testing transform.\n",
      "torch.Size([1, 28, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 28, 3, 224, 224])\n",
      "tensor([28])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  2\n",
      "Predicting on  frames_collection/recording_3\n",
      "Apply testing transform.\n",
      "torch.Size([1, 32, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 32, 3, 224, 224])\n",
      "tensor([32])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  2\n",
      "Predicting on  frames_collection/recording_4\n",
      "Apply testing transform.\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "tensor([48])\n",
      "<class 'torch.Tensor'>\n",
      "__ON__ \n",
      "['.']\n",
      "Time taken to predict (total):  3\n",
      "Predicting on  frames_collection/recording_5\n",
      "Apply testing transform.\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "tensor([48])\n",
      "<class 'torch.Tensor'>\n",
      "__ON__ \n",
      "['.']\n",
      "Time taken to predict (total):  3\n",
      "Predicting on  frames_collection/recording_6\n",
      "Apply testing transform.\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "tensor([48])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  3\n",
      "Predicting on  frames_collection/recording_7\n",
      "Apply testing transform.\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "tensor([48])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  3\n",
      "Predicting on  frames_collection/recording_8\n",
      "Apply testing transform.\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "tensor([48])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "Time taken to predict (total):  3\n"
     ]
    }
   ],
   "source": [
    "lhsequence = rhsequence = []\n",
    "sentence = ''\n",
    "trans_sentence = ''\n",
    "vcount = 0\n",
    "fcount = 0\n",
    "recordingDone = False\n",
    "frames_base_path = 'frames_collection'\n",
    "frames_path = ''\n",
    "BG_COLOR = (192, 192, 192)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280) #1280\n",
    "#cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720) #720\n",
    "#cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "\n",
    "# empties the frames_base_path location to store new set of frames\n",
    "if os.path.exists(frames_base_path):\n",
    "    shutil.rmtree(frames_base_path)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1,\n",
    "                          min_tracking_confidence=0.1,\n",
    "                          model_complexity=0, \n",
    "                          enable_segmentation=True) as holistic:\n",
    "#with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.1, min_detection_confidence=0.1, \n",
    "    #min_tracking_confidence=0.1) as hands\n",
    "    bg_image = None\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # READ CAMERA FEED\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if frame is not None:\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "\n",
    "            # DRAW LANDMARKS\n",
    "            #draw_styled_landmarks(image, results)\n",
    "\n",
    "            # EXTRACT HANDS KEYPOINTS\n",
    "            keypoints, lh_keypoints, rh_keypoints = extract_keypoints(results)        \n",
    "            lhsequence.append(lh_keypoints)\n",
    "            rhsequence.append(rh_keypoints)\n",
    "            \n",
    "\n",
    "        if frame is None and not recordingDone: \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        elif not (np.sum(lhsequence) == 0 and np.sum(rhsequence) == 0) and frame is not None:\n",
    "            \n",
    "            # with image segmentation, background as grey color\n",
    "            if False: # if True change \"frame\" to \"output_image\" in cv2.imwrite(img_path,frame) ###line 67 & 69.\n",
    "                with mp_selfie_segmentation.SelfieSegmentation( \n",
    "                                                model_selection=1) as selfie_segmentation:\n",
    "                    seg_img, seg_results = mediapipe_detection(frame, selfie_segmentation)\n",
    "                    condition = np.stack((seg_results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                    if bg_image is None:\n",
    "                        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "                        bg_image[:] = BG_COLOR\n",
    "                    output_image = np.where(condition, seg_img, bg_image)\n",
    "            \n",
    "            \n",
    "            frames_path = os.path.join(frames_base_path, 'recording_' + str(vcount))\n",
    "            img_path = frames_path + '/frame' + str(fcount) + '.png'\n",
    "            if not os.path.exists(frames_path):\n",
    "                os.makedirs(frames_path)\n",
    "                cv2.imwrite(img_path, frame)\n",
    "            else:\n",
    "                cv2.imwrite(img_path, frame) \n",
    "\n",
    "            fcount += 1\n",
    "            recordingDone = True\n",
    "            \n",
    "            if len(lhsequence) > 15:\n",
    "                lhsequence = []\n",
    "            if len(rhsequence) > 15:\n",
    "                rhsequence = []\n",
    "\n",
    "        elif recordingDone:\n",
    "            # MAKE PREDICTIONS\n",
    "            print(\"Predicting on \",frames_path)\n",
    "            resize_img_folder(frames_path+'/*.png')\n",
    "            trans_sentence, prediction_dict = get_prediction_from_frames(frames_path+'/*.png', gloss_dict)\n",
    "            \n",
    "            vcount += 1\n",
    "            fcount = 0\n",
    "            recordingDone = False\n",
    "\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (1280, 30), (0, 0, 0), -1)        \n",
    "        cv2.putText(image, str(trans_sentence), (3,20),cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Live SLR Detection', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8cb2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'SONNE poss-SEIN DANN KOMMEN REGEN MORGEN MONTAG IX AUCH MEHR WOLKE WIE HEUTE FREUNDLICH VIEL SONNE'\n",
    "['und das wird zieht ein bisschen regen auch morgen am montag da wird es wieder freundlicher und nicht \\\n",
    " mehr so viel sonnenschein wie heute .']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66813941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[0.0318, 0.0318, 0.0318,  ..., 0.1323, 0.1323, 0.1323],\n",
       "          [0.1924, 0.1924, 0.1924,  ..., 0.4570, 0.4570, 0.4570],\n",
       "          [0.6565, 0.6565, 0.6565,  ..., 0.2514, 0.2514, 0.2514],\n",
       "          ...,\n",
       "          [0.0032, 0.0032, 0.0032,  ..., 2.1680, 2.1680, 2.1680],\n",
       "          [0.3557, 0.3557, 0.3557,  ..., 0.1272, 0.1272, 0.1272],\n",
       "          [0.1804, 0.1804, 0.1804,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 'visual_features': tensor([[[0.0000, 0.0000, 1.8503,  ..., 0.0000, 0.2550, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.7584,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.8646, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.6202, 0.0000, 3.5703,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[2.0212, 0.0000, 0.9680,  ..., 0.1017, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0922, 0.0000, 1.2957,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 'feat_len': tensor([34]),\n",
       " 'conv_logits': tensor([[[-29.5987, -38.8070, -35.6067,  ..., -38.5158, -36.3762, -38.6922]],\n",
       " \n",
       "         [[-27.4180, -43.1274, -37.8721,  ..., -40.4915, -39.7351, -39.4340]],\n",
       " \n",
       "         [[-28.0168, -44.5974, -37.3108,  ..., -42.6585, -40.9782, -40.2688]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-29.3084, -46.1386, -41.8356,  ..., -42.2483, -40.5653, -41.1398]],\n",
       " \n",
       "         [[-32.6123, -47.9628, -41.4510,  ..., -43.9208, -42.6551, -43.0852]],\n",
       " \n",
       "         [[-19.5242, -37.1576, -32.8857,  ..., -34.0061, -33.2146, -33.0843]]]),\n",
       " 'sequence_logits': tensor([[[ 3.9048, -2.9966, -2.8243,  ..., -4.1717, -2.6180, -4.9804]],\n",
       " \n",
       "         [[ 6.9175, -6.2173, -2.8819,  ..., -4.2404, -4.1822, -5.3806]],\n",
       " \n",
       "         [[ 7.6772, -6.5040, -3.3360,  ..., -4.8329, -5.1579, -5.7178]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 5.7643, -9.7618, -7.6197,  ..., -8.0521, -7.4437, -5.6318]],\n",
       " \n",
       "         [[ 3.0987, -6.7437, -3.8773,  ..., -6.9471, -6.1568, -4.5658]],\n",
       " \n",
       "         [[ 5.9071, -5.3086, -4.2410,  ..., -6.1278, -5.2538, -4.2702]]]),\n",
       " 'conv_sents': [[('SONNE', 0),\n",
       "   ('HEUTE', 1),\n",
       "   ('BESONDERS', 2),\n",
       "   ('SONNE', 3),\n",
       "   ('BIS', 4),\n",
       "   ('WOCHE', 5),\n",
       "   ('DANN', 6),\n",
       "   ('KOMMEN', 7),\n",
       "   ('REGEN', 8),\n",
       "   ('MORGEN', 9),\n",
       "   ('AUCH', 10),\n",
       "   ('MEHR', 11),\n",
       "   ('WOLKE', 12),\n",
       "   ('WIE', 13)]],\n",
       " 'recognized_sents': [[('HEUTE', 0),\n",
       "   ('SONNE', 1),\n",
       "   ('BIS', 2),\n",
       "   ('GESTERN', 3),\n",
       "   ('KOMMEN', 4),\n",
       "   ('REGEN', 5),\n",
       "   ('MORGEN', 6),\n",
       "   ('WIEDER', 7),\n",
       "   ('AUCH', 8),\n",
       "   ('MEHR', 9),\n",
       "   ('WOLKE', 10),\n",
       "   ('WIE', 11)]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d58ad07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SONNE HEUTE BESONDERS SONNE BIS WOCHE DANN KOMMEN REGEN MORGEN AUCH MEHR WOLKE WIE \n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "for word, id in prediction_dict['conv_sents'][0]:\n",
    "    sentence += word + \" \"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f10350f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['die sonne zeigt sich vor allem in der neuen woche wird es auch morgen in der neuen woche wieder häufiger mal mehr mal weniger wolken .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'die sonne zeigt sich vor allem in der neuen woche wird es auch morgen in der neuen woche wieder häufiger mal mehr mal weniger wolken .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = prediction_after_transformer(sentence)\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf98346",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d49cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d1599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
