{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb84082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_class(name):\n",
    "    components = name.rsplit('.', 1)\n",
    "    print(components)\n",
    "    mod = importlib.import_module(components[0])\n",
    "    mod = getattr(mod, components[1])\n",
    "    print(mod)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd55f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slr_network', 'SLRModel']\n",
      "<class 'slr_network.SLRModel'>\n"
     ]
    }
   ],
   "source": [
    "model_args = {\"num_classes\": 1296, \"c2d_type\": \"resnet18\", \"conv_type\": 2, \"use_bn\": 1}\n",
    "loss_weights = {\"ConvCTC\": 1.0, \"SeqCTC\": 1.0, \"Dist\": 10.0}\n",
    "gloss_dict = np.load('/home/aayush/Thesis/VAC_CSLR/preprocess/phoenix2014/gloss_dict.npy', allow_pickle=True)\\\n",
    "                .item()\n",
    "\n",
    "\n",
    "model_class = import_class(\"slr_network.SLRModel\")\n",
    "model = model_class(**model_args,\n",
    "                    gloss_dict=gloss_dict,\n",
    "                    loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a34edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_weights(state_dict, modified=False):\n",
    "    state_dict = OrderedDict([(k.replace('.module', ''), v) for k, v in state_dict.items()])\n",
    "    if not modified:\n",
    "        return state_dict\n",
    "    modified_dict = dict()\n",
    "    return modified_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c54e3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('resnet18_slr_pretrained_distill25.pt', map_location=torch.device('cpu'))\n",
    "weights = modified_weights(state_dict['model_state_dict'], False)\n",
    "model.load_state_dict(weights, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a5c41c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SLRModel(\n",
       "  (conv2d): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (conv1d): TemporalConv(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1296, bias=True)\n",
       "  )\n",
       "  (temporal_model): BiLSTMLayer(\n",
       "    (rnn): LSTM(1024, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1296, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094640a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b760f13",
   "metadata": {},
   "source": [
    "# Input format modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76fbe703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from utils import video_augmentation\n",
    "import os\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1e75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleBaseFeeder(data.Dataset):\n",
    "    \n",
    "    def __init__(self, prefix, gloss_dict):\n",
    "        self.prefix = prefix # image frames path : ./01April_2010_Thursday_heute_default-5/1/*.png\n",
    "        self.dict = gloss_dict\n",
    "        self.data_aug = self.transform()\n",
    "        print(\"\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_data, label, fi = self.read_video(idx)\n",
    "        input_data, label = self.normalize(input_data, label)\n",
    "        \n",
    "        return input_data, torch.LongTensor(label), ''#self.inputs_list[idx]['original_info']\n",
    "\n",
    "    \n",
    "    def read_video(self, index, num_glosses=-1):\n",
    "        # load file info\n",
    "        fi = {\n",
    "                \"fileid\": \"01April_2010_Thursday_heute_default-5\",\n",
    "                 \"folder\": \"test/01April_2010_Thursday_heute_default-5/1/*.png\",\n",
    "                 \"signer\": \"Signer04\",\n",
    "                 \"label\": \"\",#\"ABER FREUEN  MORGEN SONNE SELTEN REGEN\",\n",
    "                 \"num_frames\": 132,\n",
    "                 \"original_info\": \"01April_2010_Thursday_heute_default-5|01April_2010_Thursday_heute_default-5/1/*.png|Signer04|ABER FREUEN  MORGEN SONNE SELTEN REGEN\"\n",
    "             }\n",
    "        \n",
    "        img_folder = os.path.join(self.prefix)\n",
    "        img_list = sorted(glob.glob(img_folder))\n",
    "        label_list = []\n",
    "        \n",
    "        for phase in fi['label'].split(\" \"):\n",
    "            if phase == '':\n",
    "                continue\n",
    "            if phase in self.dict.keys():\n",
    "                label_list.append(self.dict[phase][0])\n",
    "                \n",
    "        return [cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in img_list],\\\n",
    "            label_list, fi\n",
    "\n",
    "    \n",
    "    def normalize(self, video, label, file_id=None):\n",
    "        video, label = self.data_aug(video, label, file_id)\n",
    "        video = video.float() / 127.5 - 1\n",
    "        #print(typvideo[0][0])\n",
    "        #print(label)\n",
    "        #plt.imshow(cv2.cvtColor(video[0][0], cv2.COLOR_BGR2RGB))\n",
    "        return video, label\n",
    "\n",
    "    \n",
    "    def transform(self):\n",
    "        print(\"Apply testing transform.\")\n",
    "        return video_augmentation.Compose([\n",
    "            video_augmentation.CenterCrop(224),\n",
    "            #video_augmentation.Resize(0.5),\n",
    "            video_augmentation.ToTensor(),\n",
    "        ]) \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        #print(\"In collate_fn\")\n",
    "        batch = [item for item in sorted(batch, key=lambda x: len(x[0]), reverse=True)]\n",
    "        video, label, info = list(zip(*batch))\n",
    "        if len(video[0].shape) > 3:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n",
    "            left_pad = 6\n",
    "            right_pad = int(np.ceil(max_len / 4.0)) * 4 - max_len + 6\n",
    "            max_len = max_len + left_pad + right_pad\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid[0][None].expand(left_pad, -1, -1, -1),\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video)\n",
    "        else:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([len(vid) for vid in video])\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid), -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video).permute(0, 2, 1)\n",
    "        label_length = torch.LongTensor([len(lab) for lab in label])\n",
    "        if max(label_length) == 0:\n",
    "            return padded_video, video_length, [], [], info\n",
    "        else:\n",
    "            padded_label = []\n",
    "            for lab in label:\n",
    "                padded_label.extend(lab)\n",
    "            padded_label = torch.LongTensor(padded_label)\n",
    "            return padded_video, video_length, padded_label, label_length, info\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1 # as for prediction we just have one folder/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ea7149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply testing transform.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_dataset = SampleBaseFeeder('./dataset/dataset/01April_2010_Thursday_heute_default-5/1/*.png', gloss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b7f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "            pred_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=4, \n",
    "            collate_fn=pred_dataset.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0754d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ceddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ToTensor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34285/3764492899.py:67: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 144, 3, 224, 224])\n",
      "tensor([144])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/Thesis/VAC_CSLR/venv/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/aayush/Thesis/Thesis2022/VAC_CSLR/modules/tconv.py:44: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:450.)\n",
      "  feat_len //= 2\n"
     ]
    }
   ],
   "source": [
    "loader = data_loader\n",
    "for batch_idx, l_data in enumerate(loader):\n",
    "    print(batch_idx)\n",
    "    device = utils.GpuDataParallel()\n",
    "    vid = device.data_to_device(l_data[0])\n",
    "    vid_lgt = device.data_to_device(l_data[1])\n",
    "    label = device.data_to_device(l_data[2])\n",
    "    label_lgt = device.data_to_device(l_data[3])\n",
    "\n",
    "with torch.no_grad():\n",
    "    ret_dict = model(vid, vid_lgt, label=label, label_lgt=label_lgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd96faba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[0.0794, 0.0794, 0.0794,  ..., 0.1176, 0.1176, 0.1176],\n",
       "          [0.1410, 0.1410, 0.1410,  ..., 0.8128, 0.8128, 0.8128],\n",
       "          [1.5184, 1.5184, 1.5184,  ..., 0.9420, 0.9420, 0.9420],\n",
       "          ...,\n",
       "          [0.2038, 0.2038, 0.2038,  ..., 0.5262, 0.5262, 0.5262],\n",
       "          [0.4511, 0.4511, 0.4511,  ..., 0.5688, 0.5688, 0.5688],\n",
       "          [0.0109, 0.0109, 0.0109,  ..., 0.9165, 0.9165, 0.9165]]]),\n",
       " 'visual_features': tensor([[[1.0786, 0.0000, 0.1509,  ..., 0.6620, 0.0000, 0.0000]],\n",
       " \n",
       "         [[1.4162, 0.0000, 0.3352,  ..., 1.0442, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.5913, 0.6186, 0.0000,  ..., 0.7431, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.7912, 0.0000, 1.1050,  ..., 5.2073, 0.0000, 0.0000]],\n",
       " \n",
       "         [[3.1430, 0.3526, 0.5447,  ..., 3.7436, 0.0000, 0.6845]],\n",
       " \n",
       "         [[3.0035, 0.8708, 0.0000,  ..., 2.4030, 0.0000, 1.2275]]]),\n",
       " 'feat_len': tensor([33]),\n",
       " 'conv_logits': tensor([[[ -7.0832, -23.6861, -23.3820,  ..., -21.5323, -20.8931, -22.2800]],\n",
       " \n",
       "         [[-11.3861, -25.6734, -25.3708,  ..., -23.5683, -23.3851, -23.9227]],\n",
       " \n",
       "         [[-11.3742, -24.8564, -25.4605,  ..., -23.9173, -23.8334, -24.1564]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -9.4681, -26.7118, -23.3472,  ..., -24.1016, -23.9602, -23.5626]],\n",
       " \n",
       "         [[ -9.6064, -24.9026, -22.7562,  ..., -23.1011, -23.2158, -22.8064]],\n",
       " \n",
       "         [[ -6.8885, -24.4802, -23.4331,  ..., -22.2747, -23.1600, -22.8831]]]),\n",
       " 'sequence_logits': tensor([[[12.8576, -6.2654, -5.0469,  ..., -2.8810, -3.4519, -4.9244]],\n",
       " \n",
       "         [[11.8826, -4.7533, -5.5165,  ..., -3.1339, -4.4033, -5.3776]],\n",
       " \n",
       "         [[ 8.5965, -2.7281, -5.2417,  ..., -2.5345, -4.0328, -4.7022]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 8.1423, -8.6176, -5.3938,  ..., -5.3546, -5.0576, -4.9408]],\n",
       " \n",
       "         [[ 8.9188, -6.5306, -5.3132,  ..., -4.8384, -4.6660, -4.6066]],\n",
       " \n",
       "         [[11.5576, -5.2938, -5.6004,  ..., -4.5939, -4.6974, -4.7510]]]),\n",
       " 'conv_sents': [[('ABER', 0),\n",
       "   ('WOCHE', 1),\n",
       "   ('MORGEN', 2),\n",
       "   ('SONNE', 3),\n",
       "   ('REGEN', 4)]],\n",
       " 'recognized_sents': [[('ABER', 0),\n",
       "   ('WOCHE', 1),\n",
       "   ('MORGEN', 2),\n",
       "   ('SONNE', 3),\n",
       "   ('REGEN', 4)]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd79b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e93d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e80af755",
   "metadata": {},
   "source": [
    "# For prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60043963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]), lh, rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354bd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f45d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.6\n",
    "count=0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        print(frame.shape)\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # Prediction logic\n",
    "        keypoints, lh_keypoints, rh_keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30 and not (np.sum(lh_keypoints) == 0 and np.sum(rh_keypoints) == 0):\n",
    "            res = model(image, len(image))\n",
    "            #print(actions[np.argmax(res)])\n",
    "            print(res)\n",
    "            cv2.imwrite(\"frame_collection/frame%d.jpg\" % count, frame) \n",
    "            count += 1\n",
    "            \n",
    "            \n",
    "        # Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-3:]\n",
    "\n",
    "            \n",
    "        #cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('Live SLR Detection', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d92d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
