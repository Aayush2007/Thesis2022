{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c15f53",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb84082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'transformer-slt/')\n",
    "from onmt.translate.translator import build_translator\n",
    "import onmt\n",
    "import onmt.inputters as inputters\n",
    "from onmt.translate.beam_search import BeamSearch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_class(name):\n",
    "    components = name.rsplit('.', 1)\n",
    "    print(components)\n",
    "    mod = importlib.import_module(components[0])\n",
    "    mod = getattr(mod, components[1])\n",
    "    print(mod)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd55f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slr_network', 'SLRModel']\n",
      "<class 'slr_network.SLRModel'>\n"
     ]
    }
   ],
   "source": [
    "model_args = {\"num_classes\": 1296, \"c2d_type\": \"resnet18\", \"conv_type\": 2, \"use_bn\": 1}\n",
    "loss_weights = {\"ConvCTC\": 1.0, \"SeqCTC\": 1.0, \"Dist\": 10.0}\n",
    "gloss_dict = np.load('/home/aayush/Thesis/VAC_CSLR/preprocess/phoenix2014/gloss_dict.npy', allow_pickle=True)\\\n",
    "                .item()\n",
    "\n",
    "\n",
    "model_class = import_class(\"slr_network.SLRModel\")\n",
    "model = model_class(**model_args,\n",
    "                    gloss_dict=gloss_dict,\n",
    "                    loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a34edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_weights(state_dict, modified=False):\n",
    "    state_dict = OrderedDict([(k.replace('.module', ''), v) for k, v in state_dict.items()])\n",
    "    if not modified:\n",
    "        return state_dict\n",
    "    modified_dict = dict()\n",
    "    return modified_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c54e3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('resnet18_slr_pretrained_distill25.pt', map_location=torch.device('cpu'))\n",
    "weights = modified_weights(state_dict['model_state_dict'], False)\n",
    "model.load_state_dict(weights, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a5c41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SLRModel(\n",
       "  (conv2d): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (conv1d): TemporalConv(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1296, bias=True)\n",
       "  )\n",
       "  (temporal_model): BiLSTMLayer(\n",
       "    (rnn): LSTM(1024, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1296, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094640a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffa2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_after_transformer(glosses = 'SCHOEN ABEND TSCHUESS'):\n",
    "    opt = Namespace(align_debug=False, alpha=0.0, attn_debug=False, avg_raw_probs=False,\n",
    "                    batch_size=30, batch_type='sents', beam_size=4, beta=-0.0,\n",
    "                    block_ngram_repeat=0, config=None, coverage_penalty='none', data_type='text',\n",
    "                    dump_beam='', dynamic_dict=False, fp32=False, gpu=-1, ignore_when_blocking=[],\n",
    "                    image_channel_size=3, length_penalty='none', log_file='', log_file_level='0',\n",
    "                    max_length=100, max_sent_length=None, min_length=0, models=['transformer-slt/model_step_1600.pt'],\n",
    "                    n_best=1, output=None, phrase_table='', random_sampling_temp=1.0,\n",
    "                    random_sampling_topk=1, ratio=-0.0, replace_unk=True, report_align=False,\n",
    "                    report_time=False, sample_rate=16000, save_config=None, seed=829,\n",
    "                    shard_size=10000, share_vocab=False, src=glosses,\n",
    "                    src_dir='', stepwise_penalty=False, tgt=None, verbose=False, window='hamming',\n",
    "                    window_size=0.02, window_stride=0.01)\n",
    "\n",
    "    translator = build_translator(opt, report_score=True, out_file='')\n",
    "\n",
    "    load_test_model = onmt.model_builder.load_test_model\n",
    "    fields, model, model_opt = load_test_model(opt)\n",
    "    scorer = onmt.translate.GNMTGlobalScorer.from_opt(opt)\n",
    "\n",
    "    src_reader = inputters.str2reader[opt.data_type].from_opt(opt)\n",
    "    tgt_reader = inputters.str2reader['text'].from_opt(opt)\n",
    "\n",
    "    src_data = {\"reader\": src_reader, \"data\": [opt.src], \"dir\": ''}\n",
    "    tgt_data = {\"reader\": tgt_reader, \"data\": None, \"dir\": None}\n",
    "\n",
    "    _readers, _data, _dir = inputters.Dataset.config([('src', src_data), ('tgt', tgt_data)])\n",
    "    data = inputters.Dataset(\n",
    "            fields, readers=_readers, data=_data, dirs=_dir,\n",
    "            sort_key=inputters.str2sortkey[opt.data_type]\n",
    "            )\n",
    "\n",
    "    data_iter = inputters.OrderedIterator(\n",
    "                    dataset=data,\n",
    "                    device=torch.device(\"cpu\"),\n",
    "                    batch_size=opt.batch_size,\n",
    "                    batch_size_fn=None,\n",
    "                    train=False,\n",
    "                    sort=False,\n",
    "                    sort_within_batch=True,\n",
    "                    shuffle=False\n",
    "                )\n",
    "\n",
    "    xlation_builder = onmt.translate.TranslationBuilder(\n",
    "                            data, fields, opt.n_best, opt.replace_unk, opt.tgt,\n",
    "                            opt.phrase_table\n",
    "                        )\n",
    "\n",
    "\n",
    "    all_predictions = []\n",
    "    tgt_field = dict(fields)[\"tgt\"].base_field\n",
    "    _tgt_vocab = tgt_field.vocab\n",
    "    _tgt_eos_idx = _tgt_vocab.stoi[tgt_field.eos_token]\n",
    "    _tgt_pad_idx = _tgt_vocab.stoi[tgt_field.pad_token]\n",
    "    _tgt_bos_idx = _tgt_vocab.stoi[tgt_field.init_token]\n",
    "    _tgt_unk_idx = _tgt_vocab.stoi[tgt_field.unk_token]\n",
    "    _tgt_vocab_len = len(_tgt_vocab)\n",
    "    _exclusion_idxs = {_tgt_vocab.stoi[t] for t in opt.ignore_when_blocking}\n",
    "\n",
    "    copy_attn = model_opt.copy_attn\n",
    "\n",
    "    for batch in data_iter:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decode_strategy = BeamSearch(\n",
    "            opt.beam_size,\n",
    "            batch_size=batch.batch_size,\n",
    "            pad=_tgt_pad_idx,\n",
    "            bos=_tgt_bos_idx,\n",
    "            eos=_tgt_eos_idx,\n",
    "            n_best=opt.n_best,\n",
    "            global_scorer=scorer,\n",
    "            min_length=opt.min_length, max_length=opt.max_length,\n",
    "            return_attention=opt.attn_debug or opt.replace_unk,\n",
    "            block_ngram_repeat=opt.block_ngram_repeat,\n",
    "            exclusion_tokens=_exclusion_idxs,\n",
    "            stepwise_penalty=opt.stepwise_penalty,\n",
    "            ratio=opt.ratio)\n",
    "\n",
    "            batch_data = translator._translate_batch_with_strategy(batch, data.src_vocabs, decode_strategy)\n",
    "\n",
    "        translations = xlation_builder.from_batch(batch_data)\n",
    "        for trans in translations:\n",
    "            n_best_preds = [\" \".join(pred)\n",
    "                                    for pred in trans.pred_sents[:opt.n_best]]\n",
    "            print(n_best_preds)\n",
    "    return n_best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a28ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jetzt wünsche ich ihnen noch einen schönen abend .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/Thesis/Thesis2022/VAC_CSLR/transformer-slt/onmt/translate/beam_search.py:187: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1, 4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /pytorch/aten/src/ATen/native/Resize.cpp:23.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jetzt wünsche ich ihnen noch einen schönen abend .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample try\n",
    "temp = prediction_after_transformer('__ON__ SCHOEN ABEND TSCHUESS __OFF__')\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b760f13",
   "metadata": {},
   "source": [
    "# Input format modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76fbe703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from utils import video_augmentation\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f0fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img_path, dsize='256x256px'):\n",
    "    dsize = tuple(int(res) for res in re.findall(\"\\d+\", dsize))\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, dsize, interpolation=cv2.INTER_LANCZOS4)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize_img_folder(frames_folder_path):\n",
    "    img_list = glob.glob(frames_folder_path)\n",
    "    for img_path in img_list:\n",
    "        rs_img = resize_img(img_path)\n",
    "        cv2.imwrite(img_path, rs_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0471eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_folder_name = '01April_2010_Thursday_heute_default-5/1'\n",
    "frames_folder_path = './dataset/dataset/' + frames_folder_name + '/*.png'\n",
    "#frames_folder_path = './frames_collection/recording_0/*.png'\n",
    "resize_img_folder(frames_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf1e75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleBaseFeeder(data.Dataset):\n",
    "    \n",
    "    def __init__(self, prefix, gloss_dict):\n",
    "        self.prefix = prefix # image frames path : ./01April_2010_Thursday_heute_default-5/1/*.png\n",
    "        self.dict = gloss_dict\n",
    "        self.data_aug = self.transform()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_data, label, fi = self.read_video(idx)\n",
    "        input_data, label = self.normalize(input_data, label)\n",
    "        \n",
    "        return input_data, torch.LongTensor(label), ''#self.inputs_list[idx]['original_info']\n",
    "\n",
    "    \n",
    "    def read_video(self, index, num_glosses=-1):\n",
    "        # load file info        \n",
    "        img_folder = os.path.join(self.prefix)\n",
    "        img_list = sorted(glob.glob(img_folder))\n",
    "        label_list = []\n",
    "                \n",
    "        return [cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in img_list],\\\n",
    "            label_list, {}\n",
    "\n",
    "    \n",
    "    def normalize(self, video, label, file_id=None):\n",
    "        video, label = self.data_aug(video, label, file_id)\n",
    "        video = video.float() / 127.5 - 1\n",
    "        return video, label\n",
    "\n",
    "    \n",
    "    def transform(self):\n",
    "        print(\"Apply testing transform.\")\n",
    "        return video_augmentation.Compose([\n",
    "            video_augmentation.CenterCrop(224),\n",
    "            video_augmentation.ToTensor(),\n",
    "        ]) \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = [item for item in sorted(batch, key=lambda x: len(x[0]), reverse=True)]\n",
    "        video, label, info = list(zip(*batch))\n",
    "        if len(video[0].shape) > 3:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n",
    "            left_pad = 6\n",
    "            right_pad = int(np.ceil(max_len / 4.0)) * 4 - max_len + 6\n",
    "            max_len = max_len + left_pad + right_pad\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid[0][None].expand(left_pad, -1, -1, -1),\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video)\n",
    "        else:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([len(vid) for vid in video])\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid), -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video).permute(0, 2, 1)\n",
    "        label_length = torch.LongTensor([len(lab) for lab in label])\n",
    "        if max(label_length) == 0:\n",
    "            return padded_video, video_length, [], [], info\n",
    "        else:\n",
    "            padded_label = []\n",
    "            for lab in label:\n",
    "                padded_label.extend(lab)\n",
    "            padded_label = torch.LongTensor(padded_label)\n",
    "            return padded_video, video_length, padded_label, label_length, info\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1 # as for prediction we just have one folder/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34e5785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[993, 44]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloss_dict['TSCHUESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ea7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(frames_folder_path, gloss_dict):\n",
    "    \n",
    "    pred_dataset = SampleBaseFeeder(frames_folder_path, gloss_dict)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                pred_dataset,\n",
    "                batch_size=8,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                num_workers=4, \n",
    "                collate_fn=pred_dataset.collate_fn,\n",
    "            )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ceddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_from_frames(frames_folder_path, gloss_dict):\n",
    "    start = time.time()\n",
    "    loader = get_data_loader(frames_folder_path, gloss_dict)\n",
    "    for batch_idx, l_data in enumerate(loader):\n",
    "        print(l_data[0].shape)\n",
    "        device = utils.GpuDataParallel()\n",
    "        vid = device.data_to_device(l_data[0])\n",
    "        vid_lgt = device.data_to_device(l_data[1])\n",
    "        label = device.data_to_device(l_data[2])\n",
    "        label_lgt = device.data_to_device(l_data[3])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ret_dict = model(vid, vid_lgt, label=label, label_lgt=label_lgt)\n",
    "    \n",
    "    sentence = ''\n",
    "    for word, id in ret_dict['recognized_sents'][0]:\n",
    "        sentence += word + \" \"\n",
    "    \n",
    "    print(sentence)\n",
    "    new_sentence = prediction_after_transformer(sentence)[0]\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Time taken to predict (total): \", round(end - start))\n",
    "    return new_sentence, ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79c5aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply testing transform.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temploader = get_data_loader(frames_folder_path, gloss_dict)\n",
    "len(temploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd96faba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply testing transform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11503/701672835.py:46: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 144, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 144, 3, 224, 224])\n",
      "tensor([144])\n",
      "<class 'torch.Tensor'>\n",
      "ABER WOCHE MORGEN SONNE REGEN \n",
      "['aber in der neuen woche scheint morgen verbreitet die sonne .']\n",
      "Time taken to predict (total):  6\n"
     ]
    }
   ],
   "source": [
    "ns, ret_dict = get_prediction_from_frames(frames_folder_path, gloss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c593f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__ON__ MILD WEHEN ICH \n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "for word, id in ret_dict['recognized_sents'][0]:\n",
    "    sentence += word + \" \"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d007dc0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[0.0794, 0.0794, 0.0794,  ..., 0.1176, 0.1176, 0.1176],\n",
       "          [0.1410, 0.1410, 0.1410,  ..., 0.8128, 0.8128, 0.8128],\n",
       "          [1.5184, 1.5184, 1.5184,  ..., 0.9420, 0.9420, 0.9420],\n",
       "          ...,\n",
       "          [0.2038, 0.2038, 0.2038,  ..., 0.5262, 0.5262, 0.5262],\n",
       "          [0.4511, 0.4511, 0.4511,  ..., 0.5688, 0.5688, 0.5688],\n",
       "          [0.0109, 0.0109, 0.0109,  ..., 0.9165, 0.9165, 0.9165]]]),\n",
       " 'visual_features': tensor([[[1.0786, 0.0000, 0.1509,  ..., 0.6620, 0.0000, 0.0000]],\n",
       " \n",
       "         [[1.4162, 0.0000, 0.3352,  ..., 1.0442, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.5913, 0.6186, 0.0000,  ..., 0.7431, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.7912, 0.0000, 1.1050,  ..., 5.2073, 0.0000, 0.0000]],\n",
       " \n",
       "         [[3.1430, 0.3526, 0.5447,  ..., 3.7436, 0.0000, 0.6845]],\n",
       " \n",
       "         [[3.0035, 0.8708, 0.0000,  ..., 2.4030, 0.0000, 1.2275]]]),\n",
       " 'feat_len': tensor([33]),\n",
       " 'conv_logits': tensor([[[ -7.0832, -23.6861, -23.3820,  ..., -21.5323, -20.8931, -22.2800]],\n",
       " \n",
       "         [[-11.3861, -25.6734, -25.3708,  ..., -23.5683, -23.3851, -23.9227]],\n",
       " \n",
       "         [[-11.3742, -24.8564, -25.4605,  ..., -23.9173, -23.8334, -24.1564]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -9.4681, -26.7118, -23.3472,  ..., -24.1016, -23.9602, -23.5626]],\n",
       " \n",
       "         [[ -9.6064, -24.9026, -22.7562,  ..., -23.1011, -23.2158, -22.8064]],\n",
       " \n",
       "         [[ -6.8885, -24.4802, -23.4331,  ..., -22.2747, -23.1600, -22.8831]]]),\n",
       " 'sequence_logits': tensor([[[12.8576, -6.2654, -5.0469,  ..., -2.8810, -3.4519, -4.9244]],\n",
       " \n",
       "         [[11.8826, -4.7533, -5.5165,  ..., -3.1339, -4.4033, -5.3776]],\n",
       " \n",
       "         [[ 8.5965, -2.7281, -5.2417,  ..., -2.5345, -4.0328, -4.7022]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 8.1423, -8.6176, -5.3938,  ..., -5.3546, -5.0576, -4.9408]],\n",
       " \n",
       "         [[ 8.9188, -6.5306, -5.3132,  ..., -4.8384, -4.6660, -4.6066]],\n",
       " \n",
       "         [[11.5576, -5.2938, -5.6004,  ..., -4.5939, -4.6974, -4.7510]]]),\n",
       " 'conv_sents': [[('ABER', 0),\n",
       "   ('WOCHE', 1),\n",
       "   ('MORGEN', 2),\n",
       "   ('SONNE', 3),\n",
       "   ('REGEN', 4)]],\n",
       " 'recognized_sents': [[('ABER', 0),\n",
       "   ('WOCHE', 1),\n",
       "   ('MORGEN', 2),\n",
       "   ('SONNE', 3),\n",
       "   ('REGEN', 4)]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26e93d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fileid': '07February_2011_Monday_heute-4659', 'folder': 'train/07February_2011_Monday_heute-4659/*.png', 'signer': 'Signer07', 'label': 'SONNE VORBEI DANN KOMMEN MEHR REGEN MORGEN IX FLUSS AUCH MEHR WOLKE WIE HEUTE FREUNDLICH SONNE', 'num_frames': 134, 'original_info': '07February_2011_Monday_heute-4659|07February_2011_Monday_heute-4659/1/*.png|-1|-1|Signer07|SONNE VORBEI DANN KOMMEN MEHR REGEN MORGEN IX FLUSS AUCH MEHR WOLKE WIE HEUTE FREUNDLICH SONNE|nach sonne kommt regen so wird es auch morgen an der mosel sein eher das trübe wetter und nicht so einen herrlichen sonnenschein wie wir heute hatten'}\n"
     ]
    }
   ],
   "source": [
    "# to know a folder info looks like.\n",
    "\n",
    "inputs_list = np.load(f\"./preprocess/phoenix14t/train_info.npy\", allow_pickle=True).item()\n",
    "for x in inputs_list:\n",
    "    if x == 'prefix':\n",
    "        continue\n",
    "    if inputs_list[x]['fileid'] == frames_folder_name:\n",
    "        print(inputs_list[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6e7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f34154fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'fileid': '22November_2010_Monday_heute_default-14', 'folder': 'train/22November_2010_Monday_heute_default-14/1/*.png', 'signer': 'Signer01', 'label': '__ON__ SCHOEN ABEND MACHEN GUT TSCHUESS __OFF__', 'num_frames': 38, 'original_info': '22November_2010_Monday_heute_default-14|22November_2010_Monday_heute_default-14/1/*.png|Signer01|__ON__ SCHOEN ABEND MACHEN GUT TSCHUESS __OFF__'}\n",
      "5\n",
      "{'fileid': '30May_2011_Monday_heute_default-18', 'folder': 'train/30May_2011_Monday_heute_default-18/1/*.png', 'signer': 'Signer01', 'label': '__ON__ SCHOEN ABEND TSCHUESS __OFF__', 'num_frames': 39, 'original_info': '30May_2011_Monday_heute_default-18|30May_2011_Monday_heute_default-18/1/*.png|Signer01|__ON__ SCHOEN ABEND TSCHUESS __OFF__'}\n"
     ]
    }
   ],
   "source": [
    "inputs_list = np.load(f\"./preprocess/phoenix2014/train_info.npy\", allow_pickle=True).item()\n",
    "for x in inputs_list:\n",
    "    if x == 'prefix':\n",
    "        continue\n",
    "    \n",
    "    if inputs_list[x]['num_frames'] < 40 and 'TSCHUESS' in inputs_list[x]['label']:\n",
    "        print(len(inputs_list[x]['label'].split(' ')))\n",
    "        print(inputs_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80af755",
   "metadata": {},
   "source": [
    "# For live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2f014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba01b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50dd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60043963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              #mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             #mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             #)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7220800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]), lh, rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1f45d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on  frames_collection/recording_0\n",
      "Apply testing transform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16902/701672835.py:46: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 148, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 148, 3, 224, 224])\n",
      "tensor([148])\n",
      "<class 'torch.Tensor'>\n",
      "HEUTE FREUNDLICH SONNE DANN KOMMEN REGEN JA MORGEN MONTAG __LEFTHAND__ AUCH MEHR WOLKE WIE \n",
      "['heute nacht wird es wieder freundlicher und auch morgen montag den ganzen tag über .']\n",
      "Time taken to predict (total):  10\n"
     ]
    }
   ],
   "source": [
    "lhsequence = rhsequence = []\n",
    "sentence = ''\n",
    "trans_sentence = ''\n",
    "vcount = 0\n",
    "fcount = 0\n",
    "recordingDone = False\n",
    "frames_base_path = 'frames_collection'\n",
    "frames_path = ''\n",
    "BG_COLOR = (192, 192, 192)\n",
    "\n",
    "cap = cv2.VideoCapture('output.mp4')\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 852)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "#cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1,\n",
    "                          min_tracking_confidence=0.1,\n",
    "                          model_complexity=0, \n",
    "                          enable_segmentation=True) as holistic:\n",
    "#with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.1, min_detection_confidence=0.1, min_tracking_confidence=0.1) as hands\n",
    "    bg_image = None\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # READ CAMERA FEED\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if frame is not None:\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "\n",
    "            # DRAW LANDMARKS\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # EXTRACT HANDS KEYPOINTS\n",
    "            keypoints, lh_keypoints, rh_keypoints = extract_keypoints(results)        \n",
    "            lhsequence.append(lh_keypoints)\n",
    "            rhsequence.append(rh_keypoints)\n",
    "            \n",
    "\n",
    "        if frame is None and not recordingDone: \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        elif not (np.sum(lhsequence) == 0 and np.sum(rhsequence) == 0) and frame is not None:\n",
    "            \n",
    "            # with image segmentation, background as grey color\n",
    "            if False: # if True change frame to output_image in cv2.imwrite(img_path,frame)\n",
    "                with mp_selfie_segmentation.SelfieSegmentation( \n",
    "                                                model_selection=1) as selfie_segmentation:\n",
    "                    seg_img, seg_results = mediapipe_detection(frame, selfie_segmentation)\n",
    "                    condition = np.stack((seg_results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                    if bg_image is None:\n",
    "                        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "                        bg_image[:] = BG_COLOR\n",
    "                    output_image = np.where(condition, seg_img, bg_image)\n",
    "            \n",
    "            \n",
    "            frames_path = os.path.join(frames_base_path, 'recording_' + str(vcount))\n",
    "            img_path = frames_path + '/frame' + str(fcount) + '.png'\n",
    "            if not os.path.exists(frames_path):\n",
    "                os.makedirs(frames_path)\n",
    "                cv2.imwrite(img_path, frame)\n",
    "            else:\n",
    "                cv2.imwrite(img_path, frame) \n",
    "\n",
    "            fcount += 1\n",
    "            recordingDone = True\n",
    "            \n",
    "            if len(lhsequence) > 15:\n",
    "                lhsequence = []\n",
    "            if len(rhsequence) > 15:\n",
    "                rhsequence = []\n",
    "\n",
    "        elif recordingDone:\n",
    "            # MAKE PREDICTIONS\n",
    "            print(\"Predicting on \",frames_path)\n",
    "            resize_img_folder(frames_path+'/*.png')\n",
    "            trans_sentence, prediction_dict = get_prediction_from_frames(frames_path+'/*.png', gloss_dict)\n",
    "            \n",
    "            vcount += 1\n",
    "            fcount = 0\n",
    "            recordingDone = False\n",
    "\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (1280, 30), (0, 0, 0), -1)        \n",
    "        cv2.putText(image, str(trans_sentence), (3,20),cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Live SLR Detection', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fbe5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'SONNE poss-SEIN DANN KOMMEN REGEN MORGEN MONTAG IX AUCH MEHR WOLKE WIE HEUTE FREUNDLICH VIEL SONNE '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "66813941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[1.3008, 1.3008, 1.3008,  ..., 1.6407, 1.6407, 1.6407],\n",
       "          [1.5033, 1.5033, 1.5033,  ..., 1.6175, 1.6175, 1.6175],\n",
       "          [0.3130, 0.3130, 0.3130,  ..., 0.7975, 0.7975, 0.7975],\n",
       "          ...,\n",
       "          [0.8511, 0.8511, 0.8511,  ..., 1.4915, 1.4915, 1.4915],\n",
       "          [1.0935, 1.0935, 1.0935,  ..., 1.2195, 1.2195, 1.2195],\n",
       "          [0.8213, 0.8213, 0.8213,  ..., 0.3413, 0.3413, 0.3413]]]),\n",
       " 'visual_features': tensor([[[0.5461, 0.2666, 0.0229,  ..., 0.0000, 1.6038, 0.0000]],\n",
       " \n",
       "         [[0.6112, 0.2004, 0.1416,  ..., 0.0000, 1.7595, 0.0000]],\n",
       " \n",
       "         [[0.7808, 0.3113, 0.1024,  ..., 0.0000, 2.0837, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.2793, 1.2042, 0.0000,  ..., 0.0000, 1.3556, 0.0000]],\n",
       " \n",
       "         [[0.3082, 1.2186, 0.0000,  ..., 0.0184, 1.5509, 0.0000]],\n",
       " \n",
       "         [[0.2335, 1.0669, 0.0461,  ..., 0.3000, 1.8297, 0.0000]]]),\n",
       " 'feat_len': tensor([9]),\n",
       " 'conv_logits': tensor([[[-17.4694, -31.9186, -28.4846,  ..., -29.9022, -29.7535, -30.0468]],\n",
       " \n",
       "         [[-19.5370, -33.1330, -28.4710,  ..., -30.2560, -29.8945, -29.8216]],\n",
       " \n",
       "         [[-22.8699, -35.6188, -30.2972,  ..., -32.2336, -32.1118, -31.7839]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-12.0519, -27.2114, -24.3432,  ..., -26.6352, -26.5044, -26.3495]],\n",
       " \n",
       "         [[-15.2093, -30.6459, -27.1139,  ..., -28.6958, -28.7583, -28.0332]],\n",
       " \n",
       "         [[-20.3972, -33.7059, -29.8733,  ..., -31.1110, -31.7048, -30.8098]]]),\n",
       " 'sequence_logits': tensor([[[10.7409, -5.6608, -0.7334,  ..., -4.5093, -4.1151, -3.4047]],\n",
       " \n",
       "         [[10.6649, -6.2823, -0.7707,  ..., -4.9254, -4.8413, -3.5177]],\n",
       " \n",
       "         [[ 9.5728, -6.4869, -0.8788,  ..., -4.6883, -5.1906, -3.2831]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[16.1543, -5.7554, -1.0507,  ..., -6.6386, -5.7669, -3.6728]],\n",
       " \n",
       "         [[14.4020, -5.3575, -0.9326,  ..., -5.7134, -5.0935, -2.9163]],\n",
       " \n",
       "         [[ 9.5540, -3.4154, -0.6339,  ..., -4.1798, -4.1587, -1.9405]]]),\n",
       " 'conv_sents': [[('__LEFTHAND__', 0)]],\n",
       " 'recognized_sents': [[]]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ad07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10350f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf98346",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d49cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d1599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
