{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c15f53",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb84082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'transformer-slt/')\n",
    "from onmt.translate.translator import build_translator\n",
    "import onmt\n",
    "import onmt.inputters as inputters\n",
    "from onmt.translate.beam_search import BeamSearch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_class(name):\n",
    "    components = name.rsplit('.', 1)\n",
    "    print(components)\n",
    "    mod = importlib.import_module(components[0])\n",
    "    mod = getattr(mod, components[1])\n",
    "    print(mod)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "afd55f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slr_network', 'SLRModel']\n",
      "<class 'slr_network.SLRModel'>\n"
     ]
    }
   ],
   "source": [
    "model_args = {\"num_classes\": 1296, \"c2d_type\": \"resnet18\", \"conv_type\": 2, \"use_bn\": 1}\n",
    "loss_weights = {\"ConvCTC\": 1.0, \"SeqCTC\": 1.0, \"Dist\": 10.0}\n",
    "gloss_dict = np.load('/home/aayush/Thesis/VAC_CSLR/preprocess/phoenix2014/gloss_dict.npy', allow_pickle=True)\\\n",
    "                .item()\n",
    "\n",
    "\n",
    "model_class = import_class(\"slr_network.SLRModel\")\n",
    "model = model_class(**model_args,\n",
    "                    gloss_dict=gloss_dict,\n",
    "                    loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5a34edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_weights(state_dict, modified=False):\n",
    "    state_dict = OrderedDict([(k.replace('.module', ''), v) for k, v in state_dict.items()])\n",
    "    if not modified:\n",
    "        return state_dict\n",
    "    modified_dict = dict()\n",
    "    return modified_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7c54e3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('resnet18_slr_pretrained_distill25.pt', map_location=torch.device('cpu'))\n",
    "weights = modified_weights(state_dict['model_state_dict'], False)\n",
    "model.load_state_dict(weights, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b2a5c41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SLRModel(\n",
       "  (conv2d): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (conv1d): TemporalConv(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=1296, bias=True)\n",
       "  )\n",
       "  (temporal_model): BiLSTMLayer(\n",
       "    (rnn): LSTM(1024, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1296, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094640a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffa2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_after_transformer(glosses = 'SCHOEN ABEND TSCHUESS'):\n",
    "    opt = Namespace(align_debug=False, alpha=0.0, attn_debug=False, avg_raw_probs=False,\n",
    "                    batch_size=30, batch_type='sents', beam_size=4, beta=-0.0,\n",
    "                    block_ngram_repeat=0, config=None, coverage_penalty='none', data_type='text',\n",
    "                    dump_beam='', dynamic_dict=False, fp32=False, gpu=-1, ignore_when_blocking=[],\n",
    "                    image_channel_size=3, length_penalty='none', log_file='', log_file_level='0',\n",
    "                    max_length=100, max_sent_length=None, min_length=0, models=['transformer-slt/model_step_1600.pt'],\n",
    "                    n_best=1, output=None, phrase_table='', random_sampling_temp=1.0,\n",
    "                    random_sampling_topk=1, ratio=-0.0, replace_unk=True, report_align=False,\n",
    "                    report_time=False, sample_rate=16000, save_config=None, seed=829,\n",
    "                    shard_size=10000, share_vocab=False, src=glosses,\n",
    "                    src_dir='', stepwise_penalty=False, tgt=None, verbose=False, window='hamming',\n",
    "                    window_size=0.02, window_stride=0.01)\n",
    "\n",
    "    translator = build_translator(opt, report_score=True, out_file='')\n",
    "\n",
    "    load_test_model = onmt.model_builder.load_test_model\n",
    "    fields, model, model_opt = load_test_model(opt)\n",
    "    scorer = onmt.translate.GNMTGlobalScorer.from_opt(opt)\n",
    "\n",
    "    src_reader = inputters.str2reader[opt.data_type].from_opt(opt)\n",
    "    tgt_reader = inputters.str2reader['text'].from_opt(opt)\n",
    "\n",
    "    src_data = {\"reader\": src_reader, \"data\": [opt.src], \"dir\": ''}\n",
    "    tgt_data = {\"reader\": tgt_reader, \"data\": None, \"dir\": None}\n",
    "\n",
    "    _readers, _data, _dir = inputters.Dataset.config([('src', src_data), ('tgt', tgt_data)])\n",
    "    data = inputters.Dataset(\n",
    "            fields, readers=_readers, data=_data, dirs=_dir,\n",
    "            sort_key=inputters.str2sortkey[opt.data_type]\n",
    "            )\n",
    "\n",
    "    data_iter = inputters.OrderedIterator(\n",
    "                    dataset=data,\n",
    "                    device=torch.device(\"cpu\"),\n",
    "                    batch_size=opt.batch_size,\n",
    "                    batch_size_fn=None,\n",
    "                    train=False,\n",
    "                    sort=False,\n",
    "                    sort_within_batch=True,\n",
    "                    shuffle=False\n",
    "                )\n",
    "\n",
    "    xlation_builder = onmt.translate.TranslationBuilder(\n",
    "                            data, fields, opt.n_best, opt.replace_unk, opt.tgt,\n",
    "                            opt.phrase_table\n",
    "                        )\n",
    "\n",
    "\n",
    "    all_predictions = []\n",
    "    tgt_field = dict(fields)[\"tgt\"].base_field\n",
    "    _tgt_vocab = tgt_field.vocab\n",
    "    _tgt_eos_idx = _tgt_vocab.stoi[tgt_field.eos_token]\n",
    "    _tgt_pad_idx = _tgt_vocab.stoi[tgt_field.pad_token]\n",
    "    _tgt_bos_idx = _tgt_vocab.stoi[tgt_field.init_token]\n",
    "    _tgt_unk_idx = _tgt_vocab.stoi[tgt_field.unk_token]\n",
    "    _tgt_vocab_len = len(_tgt_vocab)\n",
    "    _exclusion_idxs = {_tgt_vocab.stoi[t] for t in opt.ignore_when_blocking}\n",
    "\n",
    "    copy_attn = model_opt.copy_attn\n",
    "\n",
    "    for batch in data_iter:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decode_strategy = BeamSearch(\n",
    "            opt.beam_size,\n",
    "            batch_size=batch.batch_size,\n",
    "            pad=_tgt_pad_idx,\n",
    "            bos=_tgt_bos_idx,\n",
    "            eos=_tgt_eos_idx,\n",
    "            n_best=opt.n_best,\n",
    "            global_scorer=scorer,\n",
    "            min_length=opt.min_length, max_length=opt.max_length,\n",
    "            return_attention=opt.attn_debug or opt.replace_unk,\n",
    "            block_ngram_repeat=opt.block_ngram_repeat,\n",
    "            exclusion_tokens=_exclusion_idxs,\n",
    "            stepwise_penalty=opt.stepwise_penalty,\n",
    "            ratio=opt.ratio)\n",
    "\n",
    "            batch_data = translator._translate_batch_with_strategy(batch, data.src_vocabs, decode_strategy)\n",
    "\n",
    "        translations = xlation_builder.from_batch(batch_data)\n",
    "        for trans in translations:\n",
    "            n_best_preds = [\" \".join(pred)\n",
    "                                    for pred in trans.pred_sents[:opt.n_best]]\n",
    "            print(n_best_preds)\n",
    "    return n_best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a28ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im westen und südwesten bleibt es weitgehend trocken .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'im westen und südwesten bleibt es weitgehend trocken .'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample try\n",
    "temp = prediction_after_transformer('WEST UND loc-NORDOST TROCKEN __OFF__ __ON__')\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b760f13",
   "metadata": {},
   "source": [
    "# Input format modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76fbe703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from utils import video_augmentation\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52f0fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img_path, dsize='256x256px'): #852x480px\n",
    "    dsize = tuple(int(res) for res in re.findall(\"\\d+\", dsize))\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, dsize, interpolation=cv2.INTER_LANCZOS4)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize_img_folder(frames_folder_path):\n",
    "    img_list = glob.glob(frames_folder_path)\n",
    "    for img_path in img_list:\n",
    "        rs_img = resize_img(img_path)\n",
    "        cv2.imwrite(img_path, rs_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bf1e75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleBaseFeeder(data.Dataset):\n",
    "    \n",
    "    def __init__(self, prefix, gloss_dict):\n",
    "        self.prefix = prefix # image frames path : ./01April_2010_Thursday_heute_default-5/1/*.png\n",
    "        self.dict = gloss_dict\n",
    "        self.data_aug = self.transform()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_data, label, fi = self.read_video(idx)\n",
    "        input_data, label = self.normalize(input_data, label)\n",
    "        \n",
    "        return input_data, torch.LongTensor(label), ''#self.inputs_list[idx]['original_info']\n",
    "\n",
    "    \n",
    "    def read_video(self, index, num_glosses=-1):\n",
    "        # load file info        \n",
    "        img_folder = os.path.join(self.prefix)\n",
    "        img_list = sorted(glob.glob(img_folder))\n",
    "        label_list = []\n",
    "                \n",
    "        return [cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in img_list],\\\n",
    "            label_list, {}\n",
    "\n",
    "    \n",
    "    def normalize(self, video, label, file_id=None):\n",
    "        video, label = self.data_aug(video, label, file_id)\n",
    "        video = video.float() / 127.5 - 1\n",
    "        return video, label\n",
    "\n",
    "    \n",
    "    def transform(self):\n",
    "        print(\"Apply testing transform.\")\n",
    "        return video_augmentation.Compose([\n",
    "            video_augmentation.CenterCrop(224),\n",
    "            video_augmentation.ToTensor(),\n",
    "        ]) \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = [item for item in sorted(batch, key=lambda x: len(x[0]), reverse=True)]\n",
    "        video, label, info = list(zip(*batch))\n",
    "        if len(video[0].shape) > 3:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n",
    "            left_pad = 6\n",
    "            right_pad = int(np.ceil(max_len / 4.0)) * 4 - max_len + 6\n",
    "            max_len = max_len + left_pad + right_pad\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid[0][None].expand(left_pad, -1, -1, -1),\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video)\n",
    "        else:\n",
    "            max_len = len(video[0])\n",
    "            video_length = torch.LongTensor([len(vid) for vid in video])\n",
    "            padded_video = [torch.cat(\n",
    "                (\n",
    "                    vid,\n",
    "                    vid[-1][None].expand(max_len - len(vid), -1),\n",
    "                )\n",
    "                , dim=0)\n",
    "                for vid in video]\n",
    "            padded_video = torch.stack(padded_video).permute(0, 2, 1)\n",
    "        label_length = torch.LongTensor([len(lab) for lab in label])\n",
    "        if max(label_length) == 0:\n",
    "            return padded_video, video_length, [], [], info\n",
    "        else:\n",
    "            padded_label = []\n",
    "            for lab in label:\n",
    "                padded_label.extend(lab)\n",
    "            padded_label = torch.LongTensor(padded_label)\n",
    "            return padded_video, video_length, padded_label, label_length, info\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1 # as for prediction we just have one folder/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e34e5785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[833, 37]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloss_dict['TSCHUESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e1ea7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(frames_folder_path, gloss_dict):\n",
    "    \n",
    "    pred_dataset = SampleBaseFeeder(frames_folder_path, gloss_dict)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "                pred_dataset,\n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                num_workers=4, \n",
    "                collate_fn=pred_dataset.collate_fn,\n",
    "            )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5ceddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_from_frames(frames_folder_path, gloss_dict):\n",
    "    start = time.time()\n",
    "    loader = get_data_loader(frames_folder_path, gloss_dict)\n",
    "    for batch_idx, l_data in enumerate(loader):\n",
    "        print(l_data[0].shape)\n",
    "        device = utils.GpuDataParallel()\n",
    "        vid = device.data_to_device(l_data[0])\n",
    "        vid_lgt = device.data_to_device(l_data[1])\n",
    "        label = device.data_to_device(l_data[2])\n",
    "        label_lgt = device.data_to_device(l_data[3])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ret_dict = model(vid, vid_lgt, label=label, label_lgt=label_lgt)\n",
    "    \n",
    "    sentence = ''\n",
    "    for word, id in ret_dict['recognized_sents'][0]:\n",
    "        sentence += word + \" \"\n",
    "    \n",
    "    print(sentence)\n",
    "    new_sentence = prediction_after_transformer(sentence)[0]\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Time taken to predict (total): \", round(end - start))\n",
    "    return new_sentence, ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0471eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames_folder_name = '05February_2010_Friday_tagesschau_default-5/1'\n",
    "#frames_folder_path = './dataset/dataset/' + frames_folder_name + '/*.png'\n",
    "frames_folder_path = './frames_collection/recording_0/*.png'\n",
    "resize_img_folder(frames_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cd96faba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply testing transform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79288/2937488101.py:46: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 88, 3, 256, 256])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 88, 3, 256, 256])\n",
      "tensor([88])\n",
      "<class 'torch.Tensor'>\n",
      "WEST UND loc-NORDOST TROCKEN __OFF__ __ON__ \n",
      "['im westen und südwesten bleibt es weitgehend trocken .']\n",
      "Time taken to predict (total):  5\n"
     ]
    }
   ],
   "source": [
    "ns, ret_dict = get_prediction_from_frames(frames_folder_path, gloss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c593f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__ON__ WEST UND NORD MEHR TROCKEN __OFF__ \n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "for word, id in ret_dict['recognized_sents'][0]:\n",
    "    sentence += word + \" \"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d007dc0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[0.2540, 0.2540, 0.2540,  ..., 0.5625, 0.5625, 0.5625],\n",
       "          [0.0838, 0.0838, 0.0838,  ..., 0.3167, 0.3167, 0.3167],\n",
       "          [0.3522, 0.3522, 0.3522,  ..., 0.7051, 0.7051, 0.7051],\n",
       "          ...,\n",
       "          [0.2195, 0.2195, 0.2195,  ..., 0.3002, 0.3002, 0.3002],\n",
       "          [2.5052, 2.5052, 2.5052,  ..., 0.9088, 0.9088, 0.9088],\n",
       "          [1.3630, 1.3630, 1.3630,  ..., 0.2656, 0.2656, 0.2656]]]),\n",
       " 'visual_features': tensor([[[0.8407, 0.0000, 1.3410,  ..., 0.0000, 1.8610, 0.0037]],\n",
       " \n",
       "         [[1.2409, 0.0000, 0.1824,  ..., 0.0000, 2.7254, 0.7096]],\n",
       " \n",
       "         [[1.7770, 0.0000, 0.0000,  ..., 0.0000, 1.7524, 0.4224]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.3814, 0.0000, 0.0000,  ..., 0.4199, 0.5441, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 1.1069, 1.1035, 1.2560]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 1.3379, 1.4883, 0.9556]]]),\n",
       " 'feat_len': tensor([34]),\n",
       " 'conv_logits': tensor([[[ -6.0831, -26.3708, -21.5106,  ..., -22.5799, -21.8004, -23.2185]],\n",
       " \n",
       "         [[-12.5034, -27.4498, -23.0626,  ..., -24.0699, -23.0352, -24.4721]],\n",
       " \n",
       "         [[ -9.7074, -26.7193, -25.0167,  ..., -23.8187, -23.6425, -24.7206]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-16.8697, -32.1372, -28.7616,  ..., -28.0110, -26.7758, -28.5804]],\n",
       " \n",
       "         [[-17.6510, -33.5029, -29.3712,  ..., -29.5602, -28.4756, -30.4172]],\n",
       " \n",
       "         [[-18.4775, -32.1773, -28.5537,  ..., -27.7962, -27.5872, -29.4171]]]),\n",
       " 'sequence_logits': tensor([[[ 9.2607, -6.6365, -2.5297,  ..., -3.3613, -2.9689, -4.2002]],\n",
       " \n",
       "         [[ 7.2199, -6.6895, -2.8443,  ..., -3.4959, -2.9883, -5.0099]],\n",
       " \n",
       "         [[10.6339, -7.1850, -5.9128,  ..., -4.0579, -4.9331, -6.4218]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 5.6721, -8.1911, -6.1469,  ..., -5.8400, -3.3717, -6.2194]],\n",
       " \n",
       "         [[ 9.2100, -7.2487, -4.6678,  ..., -5.7450, -3.8880, -6.2757]],\n",
       " \n",
       "         [[ 5.5925, -5.7567, -4.0333,  ..., -4.3669, -3.3360, -4.9079]]]),\n",
       " 'conv_sents': [[('SONNE', 0),\n",
       "   ('DANN', 1),\n",
       "   ('KOMMEN', 2),\n",
       "   ('REGEN', 3),\n",
       "   ('MORGEN', 4),\n",
       "   ('IX', 5),\n",
       "   ('AUCH', 6),\n",
       "   ('MEHR', 7),\n",
       "   ('WOLKE', 8),\n",
       "   ('WIE', 9),\n",
       "   ('HEUTE', 10),\n",
       "   ('FREUNDLICH', 11),\n",
       "   ('VIEL', 12),\n",
       "   ('SONNE', 13)]],\n",
       " 'recognized_sents': [[('SONNE', 0),\n",
       "   ('poss-SEIN', 1),\n",
       "   ('DANN', 2),\n",
       "   ('KOMMEN', 3),\n",
       "   ('REGEN', 4),\n",
       "   ('MORGEN', 5),\n",
       "   ('MONTAG', 6),\n",
       "   ('IX', 7),\n",
       "   ('AUCH', 8),\n",
       "   ('MEHR', 9),\n",
       "   ('WOLKE', 10),\n",
       "   ('WIE', 11),\n",
       "   ('HEUTE', 12),\n",
       "   ('FREUNDLICH', 13),\n",
       "   ('VIEL', 14),\n",
       "   ('SONNE', 15)]]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23cd79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26e93d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fileid': '20February_2010_Saturday_tagesschau-7899', 'folder': 'test/20February_2010_Saturday_tagesschau-7899/*.png', 'signer': 'Signer04', 'label': 'NORD FLUSS MINUS EINS FLUSS PLUS ACHT', 'num_frames': 110, 'original_info': '20February_2010_Saturday_tagesschau-7899|20February_2010_Saturday_tagesschau-7899/1/*.png|-1|-1|Signer04|NORD FLUSS MINUS EINS FLUSS PLUS ACHT|nordöstlich der elbe morgen örtlich minus ein am oberrhein bis plus acht grad'}\n"
     ]
    }
   ],
   "source": [
    "# to know a folder info looks like.\n",
    "\n",
    "inputs_list = np.load(f\"./preprocess/phoenix14t/test_info.npy\", allow_pickle=True).item()\n",
    "for x in inputs_list:\n",
    "    if x == 'prefix':\n",
    "        continue\n",
    "    if inputs_list[x]['fileid'] == frames_folder_name:\n",
    "        print(inputs_list[x])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6e7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f34154fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'fileid': '22November_2010_Monday_heute_default-14', 'folder': 'train/22November_2010_Monday_heute_default-14/1/*.png', 'signer': 'Signer01', 'label': '__ON__ SCHOEN ABEND MACHEN GUT TSCHUESS __OFF__', 'num_frames': 38, 'original_info': '22November_2010_Monday_heute_default-14|22November_2010_Monday_heute_default-14/1/*.png|Signer01|__ON__ SCHOEN ABEND MACHEN GUT TSCHUESS __OFF__'}\n",
      "5\n",
      "{'fileid': '30May_2011_Monday_heute_default-18', 'folder': 'train/30May_2011_Monday_heute_default-18/1/*.png', 'signer': 'Signer01', 'label': '__ON__ SCHOEN ABEND TSCHUESS __OFF__', 'num_frames': 39, 'original_info': '30May_2011_Monday_heute_default-18|30May_2011_Monday_heute_default-18/1/*.png|Signer01|__ON__ SCHOEN ABEND TSCHUESS __OFF__'}\n"
     ]
    }
   ],
   "source": [
    "inputs_list = np.load(f\"./preprocess/phoenix2014/train_info.npy\", allow_pickle=True).item()\n",
    "for x in inputs_list:\n",
    "    if x == 'prefix':\n",
    "        continue\n",
    "    \n",
    "    if inputs_list[x]['num_frames'] < 40 and 'TSCHUESS' in inputs_list[x]['label']:\n",
    "        print(len(inputs_list[x]['label'].split(' ')))\n",
    "        print(inputs_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80af755",
   "metadata": {},
   "source": [
    "# For live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b2f014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba01b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50dd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60043963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              #mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             #mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             #)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7220800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]), lh, rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e1f45d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on  frames_collection/recording_0\n",
      "Apply testing transform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79288/701672835.py:46: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  video_length = torch.LongTensor([np.ceil(len(vid) / 4.0) * 4 + 12 for vid in video])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 88, 3, 224, 224])\n",
      "In slr_network.py forward:\n",
      "torch.Size([1, 88, 3, 224, 224])\n",
      "tensor([88])\n",
      "<class 'torch.Tensor'>\n",
      "WEST UND loc-NORDOST MEHR TROCKEN __OFF__ __ON__ \n",
      "['im westen und nordosten bleibt es meist trocken .']\n",
      "Time taken to predict (total):  7\n"
     ]
    }
   ],
   "source": [
    "lhsequence = rhsequence = []\n",
    "sentence = ''\n",
    "trans_sentence = ''\n",
    "vcount = 0\n",
    "fcount = 0\n",
    "recordingDone = False\n",
    "frames_base_path = 'frames_collection'\n",
    "frames_path = ''\n",
    "BG_COLOR = (192, 192, 192)\n",
    "\n",
    "cap = cv2.VideoCapture('output3.mp4')\n",
    "#cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280) #1280\n",
    "#cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720) #720\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "if os.path.exists(frames_base_path):\n",
    "    shutil.rmtree(frames_base_path)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1,\n",
    "                          min_tracking_confidence=0.1,\n",
    "                          model_complexity=0, \n",
    "                          enable_segmentation=True) as holistic:\n",
    "#with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.1, min_detection_confidence=0.1, min_tracking_confidence=0.1) as hands\n",
    "    bg_image = None\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # READ CAMERA FEED\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if frame is not None:\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "\n",
    "            # DRAW LANDMARKS\n",
    "            #draw_styled_landmarks(image, results)\n",
    "\n",
    "            # EXTRACT HANDS KEYPOINTS\n",
    "            keypoints, lh_keypoints, rh_keypoints = extract_keypoints(results)        \n",
    "            lhsequence.append(lh_keypoints)\n",
    "            rhsequence.append(rh_keypoints)\n",
    "            \n",
    "\n",
    "        if frame is None and not recordingDone: \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        elif not (np.sum(lhsequence) == 0 and np.sum(rhsequence) == 0) and frame is not None:\n",
    "            \n",
    "            # with image segmentation, background as grey color\n",
    "            if False: # if True change frame to output_image in cv2.imwrite(img_path,frame)\n",
    "                with mp_selfie_segmentation.SelfieSegmentation( \n",
    "                                                model_selection=1) as selfie_segmentation:\n",
    "                    seg_img, seg_results = mediapipe_detection(frame, selfie_segmentation)\n",
    "                    condition = np.stack((seg_results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                    if bg_image is None:\n",
    "                        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "                        bg_image[:] = BG_COLOR\n",
    "                    output_image = np.where(condition, seg_img, bg_image)\n",
    "            \n",
    "            \n",
    "            frames_path = os.path.join(frames_base_path, 'recording_' + str(vcount))\n",
    "            img_path = frames_path + '/frame' + str(fcount) + '.png'\n",
    "            if not os.path.exists(frames_path):\n",
    "                os.makedirs(frames_path)\n",
    "                cv2.imwrite(img_path, frame)\n",
    "            else:\n",
    "                cv2.imwrite(img_path, frame) \n",
    "\n",
    "            fcount += 1\n",
    "            recordingDone = True\n",
    "            \n",
    "            if len(lhsequence) > 15:\n",
    "                lhsequence = []\n",
    "            if len(rhsequence) > 15:\n",
    "                rhsequence = []\n",
    "\n",
    "        elif recordingDone:\n",
    "            # MAKE PREDICTIONS\n",
    "            print(\"Predicting on \",frames_path)\n",
    "            resize_img_folder(frames_path+'/*.png')\n",
    "            trans_sentence, prediction_dict = get_prediction_from_frames(frames_path+'/*.png', gloss_dict)\n",
    "            \n",
    "            vcount += 1\n",
    "            fcount = 0\n",
    "            recordingDone = False\n",
    "\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (1280, 30), (0, 0, 0), -1)        \n",
    "        cv2.putText(image, str(trans_sentence), (3,20),cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Live SLR Detection', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8fbe5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'SONNE poss-SEIN DANN KOMMEN REGEN MORGEN MONTAG IX AUCH MEHR WOLKE WIE HEUTE FREUNDLICH VIEL SONNE'\n",
    "['und das wird zieht ein bisschen regen auch morgen am montag da wird es wieder freundlicher und nicht \\\n",
    " mehr so viel sonnenschein wie heute .']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66813941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framewise_features': tensor([[[0.2630, 0.2630, 0.2630,  ..., 0.5847, 0.5847, 0.5847],\n",
       "          [0.0652, 0.0652, 0.0652,  ..., 0.8157, 0.8157, 0.8157],\n",
       "          [0.3959, 0.3959, 0.3959,  ..., 0.0551, 0.0551, 0.0551],\n",
       "          ...,\n",
       "          [0.2398, 0.2398, 0.2398,  ..., 2.0034, 2.0034, 2.0034],\n",
       "          [2.4539, 2.4539, 2.4539,  ..., 0.3210, 0.3210, 0.3210],\n",
       "          [1.3474, 1.3474, 1.3474,  ..., 0.7770, 0.7770, 0.7770]]]),\n",
       " 'visual_features': tensor([[[0.9912, 0.0000, 0.3630,  ..., 0.0000, 1.7872, 0.0000]],\n",
       " \n",
       "         [[2.1291, 0.0000, 0.0000,  ..., 0.0000, 1.1432, 0.0000]],\n",
       " \n",
       "         [[1.8536, 0.0000, 0.0000,  ..., 0.0000, 0.9565, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.3557, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[1.6911, 0.0000, 0.0000,  ..., 0.3432, 0.0000, 0.0000]],\n",
       " \n",
       "         [[1.0905, 0.0000, 0.2072,  ..., 0.3301, 0.0000, 0.0000]]]),\n",
       " 'feat_len': tensor([34]),\n",
       " 'conv_logits': tensor([[[-12.3489, -29.0998, -24.4421,  ..., -25.9488, -24.4942, -26.2271]],\n",
       " \n",
       "         [[-10.3750, -30.5051, -24.5208,  ..., -25.8725, -24.0324, -24.7527]],\n",
       " \n",
       "         [[-16.9765, -33.1261, -26.4976,  ..., -27.4151, -25.3594, -25.2845]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -8.4630, -28.7596, -24.9461,  ..., -24.2606, -23.2107, -23.5475]],\n",
       " \n",
       "         [[-26.1573, -35.7670, -31.5671,  ..., -31.7207, -31.1359, -31.3010]],\n",
       " \n",
       "         [[-14.8483, -31.1448, -26.2902,  ..., -26.6041, -25.8445, -25.6886]]]),\n",
       " 'sequence_logits': tensor([[[  8.3786,  -7.2978,  -3.6574,  ...,  -4.3328,  -2.6381,  -4.2779]],\n",
       " \n",
       "         [[ 10.2498,  -9.6419,  -5.9636,  ...,  -6.4036,  -3.9167,  -5.3440]],\n",
       " \n",
       "         [[  2.2323,  -8.5634,  -5.1599,  ...,  -5.7419,  -3.1601,  -4.2306]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 11.2915, -10.4986,  -7.0580,  ...,  -6.9611,  -5.8827,  -5.8291]],\n",
       " \n",
       "         [[  3.5822,  -8.8208,  -5.2329,  ...,  -6.1735,  -5.4974,  -4.7908]],\n",
       " \n",
       "         [[  7.8740,  -7.9362,  -5.4878,  ...,  -6.5433,  -5.7565,  -4.7837]]]),\n",
       " 'conv_sents': [[('HEUTE', 0),\n",
       "   ('VIEL', 1),\n",
       "   ('SONNE', 2),\n",
       "   ('DANN', 3),\n",
       "   ('KOMMEN', 4),\n",
       "   ('REGEN', 5),\n",
       "   ('MORGEN', 6),\n",
       "   ('IX', 7),\n",
       "   ('AUCH', 8),\n",
       "   ('MEHR', 9),\n",
       "   ('WOLKE', 10),\n",
       "   ('WIE', 11),\n",
       "   ('HEUTE', 12)]],\n",
       " 'recognized_sents': [[('HEUTE', 0),\n",
       "   ('FREUNDLICH', 1),\n",
       "   ('SONNE', 2),\n",
       "   ('DANN', 3),\n",
       "   ('KOMMEN', 4),\n",
       "   ('REGEN', 5),\n",
       "   ('MORGEN', 6),\n",
       "   ('IX', 7),\n",
       "   ('AUCH', 8),\n",
       "   ('MEHR', 9),\n",
       "   ('WOLKE', 10),\n",
       "   ('WIE', 11),\n",
       "   ('HEUTE', 12)]]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ad07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10350f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf98346",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d49cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d1599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
